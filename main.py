import asyncio
import hashlib
import html
import json
import logging
import os
import random
import re
from collections import deque
from datetime import datetime, timedelta
from typing import List, Dict, Set, Optional, Tuple
from urllib.parse import urlparse, urlunparse, parse_qsl, urlencode

import aiohttp
import feedparser
import pytz
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from telegram import Bot, error

# ===================== ENV & LOG =====================

load_dotenv()

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    handlers=[
        logging.FileHandler('bot.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)
logging.getLogger("httpx").setLevel(logging.WARNING)

# ===================== CONST =====================

RSS_SOURCES = [
    "https://www.finam.ru/analytics/rsspoint/",
    "https://www.cbr.ru/rss/eventrss",
    "https://www.vedomosti.ru/rss/rubric/finance/banks.xml",
    "https://arb.ru/rss/news/",
    "https://www.bfm.ru/news.rss?rubric=28",
    "https://ria.ru/export/rss2/archive/index.xml",
]

BACKUP_SOURCES = [
    "https://www.interfax.ru/rss.asp",
]

BOT_TOKEN = os.getenv("BOT_TOKEN")
CHANNEL_ID = os.getenv("CHANNEL_ID")

if not BOT_TOKEN:
    raise ValueError("‚ùå BOT_TOKEN –Ω–µ –∑–∞–¥–∞–Ω –≤ .env —Ñ–∞–π–ª–µ!")
if not CHANNEL_ID:
    raise ValueError("‚ùå CHANNEL_ID –Ω–µ –∑–∞–¥–∞–Ω –≤ .env —Ñ–∞–π–ª–µ!")

if CHANNEL_ID.lstrip('-').isdigit() and len(CHANNEL_ID.lstrip('-')) >= 10 and not CHANNEL_ID.startswith('-100'):
    CHANNEL_ID = '-100' + CHANNEL_ID.lstrip('-')

MAX_POSTS_PER_DAY = 5
MAX_CONTENT_LENGTH = 800
MIN_CONTENT_LENGTH = 100

USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 12_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.1 Safari/605.1.15',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0 Safari/537.36',
]

HEADERS = {'User-Agent': random.choice(USER_AGENTS)}

FINANCE_KEYWORDS = [
    '–±–∞–Ω–∫', '–∫—Ä–µ–¥–∏—Ç', '–∏–ø–æ—Ç–µ–∫–∞', '–≤–∫–ª–∞–¥', '–¥–µ–ø–æ–∑–∏—Ç', '–∞–∫—Ü–∏—è', '–æ–±–ª–∏–≥–∞—Ü–∏—è',
    '—Ä—É–±–ª—å', '–¥–æ–ª–ª–∞—Ä', '–µ–≤—Ä–æ', '–∏–Ω—Ñ–ª—è—Ü–∏—è', '—Å—Ç–∞–≤–∫–∞', '—Ü–±', '—Ñ—Ä—Å', '–±–∏—Ä–∂–∞',
    '–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞', '–Ω–µ—Ñ—Ç—å', '–≥–∞–∑', '—ç–∫–æ–Ω–æ–º–∏–∫–∞', '—Ä—ã–Ω–æ–∫', '–∏–Ω–≤–µ—Å—Ç', '—Ñ–∏–Ω–∞–Ω—Å',
    '–ª–∏–∫–≤–∏–¥–Ω–æ—Å—Ç—å', '–¥–∏–≤–∏–¥–µ–Ω–¥', '–∫—Ä–∏–∑–∏—Å', '—Å–∞–Ω–∫—Ü–∏–∏', '—Ä–µ–≥—É–ª—è—Ç–æ—Ä', '—Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π –±–∞–Ω–∫',
    '–∫—Ä–µ–¥–∏—Ç–Ω–∞—è', '–∑–∞–µ–º', '–∑–∞–π–º', '—Ä–µ—Ñ–∏–Ω–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–∏–µ', '—Å–±–µ—Ä–µ–∂–µ–Ω–∏—è', '—Ñ–æ–Ω–¥–æ–≤—ã–π',
    '–≤–∞–ª—é—Ç–Ω—ã–π', '–∫—É—Ä—Å', '–æ–±–º–µ–Ω', '–ø–ª–∞—Ç–µ–∂', '–ø–µ—Ä–µ–≤–æ–¥', '–∫–∞—Ä—Ç–∞', '–±—Ä–æ–∫–µ—Ä',
    '–∫–æ—Ç–∏—Ä–æ–≤–∫–∏', '–∏–Ω–¥–µ–∫—Å', '–∫–∞–ø–∏—Ç–∞–ª–∏–∑–∞—Ü–∏—è', '–∞–∫—Ç–∏–≤', '–ø–∞—Å—Å–∏–≤', '–±–∞–ª–∞–Ω—Å', '–æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å',
    '–ø—Ä–∏–±—ã–ª—å', '—É–±—ã—Ç–æ–∫', '–≤—ã–ø–ª–∞—Ç—ã', '–∫–≤–∞—Ä—Ç–∞–ª—å–Ω—ã–π', '–≥–æ–¥–æ–≤–æ–π', '–Ω–∞–¥–∑–æ—Ä', '–ª–∏—Ü–µ–Ω–∑–∏—è',
    '—Å–∞–Ω–∞—Ü–∏—è', '–±–∞–Ω–∫—Ä–æ—Ç—Å—Ç–≤–æ', '—Ñ–æ—Ä–µ–∫—Å', '–∏–Ω–≤–µ—Å—Ç–æ—Ä', '–ø–æ—Ä—Ç—Ñ–µ–ª—å', '—Ä–∏—Å–∫', '–¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å',
    '–ø—Ä–æ—Ü–µ–Ω—Ç', '–∫–ª—é—á–µ–≤–∞—è —Å—Ç–∞–≤–∫–∞', '–º–æ–Ω–µ—Ç–∞—Ä–Ω—ã–π', '—Ñ–∏—Å–∫–∞–ª—å–Ω—ã–π', '–±—é–¥–∂–µ—Ç', '–Ω–∞–ª–æ–≥', '—Ç–∞—Ä–∏—Ñ',
    '—Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ', '–ø–µ–Ω—Å–∏–æ–Ω–Ω—ã–π', '–ª–∏–∑–∏–Ω–≥', '—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥'
]

EXCLUDE_PATTERNS = [
    r'–±–∞–Ω–∫–µ—Ç', r'—Å—Ç–∞–≤–∫[–∞—É–∏]\s+–Ω–∞', r'–∫—Ä–µ–¥–∏—Ç\s+–¥–æ–≤–µ—Ä–∏', r'–≤–∏–¥–µ–æ\s*—Ä–æ–ª–∏–∫',
    r'—Ñ–æ—Ç–æ\s*—Ä–µ–ø–æ—Ä—Ç–∞–∂', r'–≥–∞–ª–µ—Ä–µ—è', r'–∞–Ω–æ–Ω—Å', r'—Ç—Ä–∞–Ω—Å–ª—è—Ü', r'–æ–Ω–ª–∞–π–Ω',
    r'–±–ª–æ–≥', r'–º–Ω–µ–Ω–∏–µ', r'–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π', r'–æ–ø—Ä–æ—Å', r'—Ä–µ–π—Ç–∏–Ω–≥'
]

KEYWORDS_TO_HASHTAGS = {
    "–±–∞–Ω–∫": ["#–±–∞–Ω–∫–∏", "#—Ñ–∏–Ω–∞–Ω—Å—ã", "#–±–∞–Ω–∫–æ–≤—Å–∫–∏–π–°–µ–∫—Ç–æ—Ä"],
    "–∫—Ä–µ–¥–∏—Ç": ["#–∫—Ä–µ–¥–∏—Ç", "#–∫—Ä–µ–¥–∏—Ç–æ–≤–∞–Ω–∏–µ", "#–∑–∞–π–º—ã"],
    "–∏–ø–æ—Ç–µ–∫–∞": ["#–∏–ø–æ—Ç–µ–∫–∞", "#–Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å", "#–∂–∏–ª—å–µ"],
    "–≤–∫–ª–∞–¥": ["#–≤–∫–ª–∞–¥—ã", "#–¥–µ–ø–æ–∑–∏—Ç—ã", "#—Å–±–µ—Ä–µ–∂–µ–Ω–∏—è"],
    "–∞–∫—Ü–∏—è": ["#–∞–∫—Ü–∏–∏", "#–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏", "#—Ñ–æ–Ω–¥–æ–≤—ã–π–†—ã–Ω–æ–∫"],
    "–æ–±–ª–∏–≥–∞—Ü–∏—è": ["#–æ–±–ª–∏–≥–∞—Ü–∏–∏", "#–±–æ–Ω–¥—Å", "#–¥–æ–ª–≥–æ–≤—ã–µ–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã"],
    "—Ä—É–±–ª—å": ["#—Ä—É–±–ª—å", "#–≤–∞–ª—é—Ç–∞", "#–∫—É—Ä—Å–†—É–±–ª—è"],
    "–¥–æ–ª–ª–∞—Ä": ["#–¥–æ–ª–ª–∞—Ä", "#USD", "#–≤–∞–ª—é—Ç–∞"],
    "–µ–≤—Ä–æ": ["#–µ–≤—Ä–æ", "#EUR", "#–≤–∞–ª—é—Ç–∞"],
    "–∏–Ω—Ñ–ª—è—Ü–∏—è": ["#–∏–Ω—Ñ–ª—è—Ü–∏—è", "#—Ü–µ–Ω—ã", "#—ç–∫–æ–Ω–æ–º–∏–∫–∞"],
    "–∫–ª—é—á–µ–≤–∞—è —Å—Ç–∞–≤–∫–∞": ["#–∫–ª—é—á–µ–≤–∞—è–°—Ç–∞–≤–∫–∞", "#–¶–ë", "#–ø—Ä–æ—Ü–µ–Ω—Ç–Ω–∞—è–°—Ç–∞–≤–∫–∞"],
    "—Ü–±": ["#–¶–µ–Ω—Ç—Ä–æ–±–∞–Ω–∫", "#—Ä–µ–≥—É–ª—è—Ç–æ—Ä", "#–±–∞–Ω–∫–†–æ—Å—Å–∏–∏"],
    "—Ñ—Ä—Å": ["#–§–†–°", "#–°–®–ê"],
    "–±–∏—Ä–∂–∞": ["#–±–∏—Ä–∂–∞", "#—Ç—Ä–µ–π–¥–∏–Ω–≥", "#—Ñ–æ–Ω–¥–æ–≤—ã–π–†—ã–Ω–æ–∫"],
    "–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞": ["#–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞", "#–±–∏—Ç–∫–æ–∏–Ω", "#–±–ª–æ–∫—á–µ–π–Ω"],
    "–Ω–µ—Ñ—Ç—å": ["#–Ω–µ—Ñ—Ç—å", "#—ç–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞"],
    "–≥–∞–∑": ["#–≥–∞–∑", "#—ç–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞"],
    "—ç–∫–æ–Ω–æ–º–∏–∫–∞": ["#—ç–∫–æ–Ω–æ–º–∏–∫–∞", "#–º–∞–∫—Ä–æ—ç–∫–æ–Ω–æ–º–∏–∫–∞"],
}

TOPIC_TO_EMOJI = {
    "–±–∞–Ω–∫": "üè¶", "–∫—Ä–µ–¥–∏—Ç": "üí≥", "–∏–ø–æ—Ç–µ–∫–∞": "üè†", "–≤–∫–ª–∞–¥": "üí∞",
    "–∞–∫—Ü–∏—è": "üìà", "–æ–±–ª–∏–≥–∞—Ü–∏—è": "üìä", "—Ä—É–±–ª—å": "‚ÇΩ", "–¥–æ–ª–ª–∞—Ä": "üíµ", "–µ–≤—Ä–æ": "üí∂",
    "–∏–Ω—Ñ–ª—è—Ü–∏—è": "üìâ", "–∫–ª—é—á–µ–≤–∞—è —Å—Ç–∞–≤–∫–∞": "üìå", "—Ü–±": "üá∑üá∫", "—Ñ—Ä—Å": "üá∫üá∏",
    "–±–∏—Ä–∂–∞": "üìä", "–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞": "‚Çø", "–Ω–µ—Ñ—Ç—å": "üõ¢Ô∏è", "–≥–∞–∑": "üî•",
    "—ç–∫–æ–Ω–æ–º–∏–∫–∞": "üåê", "—Ä—ã–Ω–æ–∫": "ü§ù", "–∏–Ω–≤–µ—Å—Ç": "üíº", "–ª–∏–∫–≤–∏–¥–Ω–æ—Å—Ç—å": "üíß",
    "–¥–∏–≤–∏–¥–µ–Ω–¥": "üéÅ", "–∫—Ä–∏–∑–∏—Å": "‚ö†Ô∏è", "—Å–∞–Ω–∫—Ü–∏–∏": "üö´", "—Ä–µ–≥—É–ª—è—Ç–æ—Ä": "üëÆ",
}

RUS_MONTHS = r'(—è–Ω–≤–∞—Ä[—å—è]|—Ñ–µ–≤—Ä–∞–ª[—å—è]|–º–∞—Ä—Ç[–∞–µ]?|–∞–ø—Ä–µ–ª[—å—è]|–º–∞[–µ—è]|–∏—é–Ω[—å—è]|–∏—é–ª[—å—è]|–∞–≤–≥—É—Å—Ç[–∞–µ]?|—Å–µ–Ω—Ç—è–±—Ä[—å—è]|–æ–∫—Ç—è–±—Ä[—å—è]|–Ω–æ—è–±—Ä[—å—è]|–¥–µ–∫–∞–±—Ä[—å—è])'

# –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –≤—Å–µ—Ö —Å–∞–π—Ç–æ–≤
UNIVERSAL_SELECTORS = [
    '.article-content', '.post-content', '.entry-content', 
    '.article__body', '.article-body', 'article', '.content',
    '.news-text', '.news-content', '.text', '.story__content',
    '.article-text', '.post-body', '.entry-body'
]

# ===================== UTILS =====================

def canon_url(url: str) -> str:
    """–£–¥–∞–ª—è–µ–º UTM –∏ –ø—Ä–æ—á–∏–µ –º—É—Å–æ—Ä–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Å—Å—ã–ª–∫—É."""
    try:
        u = urlparse(url)
        q = [(k, v) for k, v in parse_qsl(u.query, keep_blank_values=True)
             if not k.lower().startswith('utm')
             and k.lower() not in {'fbclid', 'gclid', 'yclid', 'utm_referrer'}]
        return urlunparse((u.scheme, u.netloc, u.path, u.params, urlencode(q, doseq=True), ''))
    except Exception:
        return url

def domain_of(url: str) -> str:
    try:
        return urlparse(url).netloc.lower()
    except Exception:
        return url

def normalize_title(title: str) -> str:
    """–£–±–∏—Ä–∞–µ–º –¥–∞—Ç—ã/—Ö–≤–æ—Å—Ç—ã –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞."""
    if not title:
        return ""
    title = re.sub(rf'\b\d{{1,2}}\s+{RUS_MONTHS}\s+\d{{4}}\s*–≥?\.?,?\s*', ' ', title, flags=re.IGNORECASE)
    title = re.sub(r'\b\d{1,2}[./-]\d{1,2}[./-]\d{2,4}\b', ' ', title)
    title = re.sub(r'\s*[-‚Äî‚Äì]\s*[^\n]+$', '', title).strip()
    return re.sub(r'\s+', ' ', title).strip()

def strip_byline_dates_everywhere(text: str) -> str:
    """–£–±–∏—Ä–∞–µ–º –∞–≤—Ç–æ—Ä–æ–≤/–¥–∞—Ç—ã/—Å–ª—É–∂–µ–±–Ω—ã–µ –≤—Å—Ç–∞–≤–∫–∏ –≤ –ª—é–±–æ–º –º–µ—Å—Ç–µ —Ç–µ–∫—Å—Ç–∞."""
    if not text:
        return ""
    patterns = [
        rf'\b\d{{1,2}}\s+{RUS_MONTHS}\s+\d{{4}}\b',
        r'\b\d{1,2}[:.]\d{2}\b',
        r'\b\d{1,2}[./-]\d{1,2}[./-]\d{2,4}\b',
        r'(?:–ê–≤—Ç–æ—Ä|–ö–æ—Ä—Ä–µ—Å–ø–æ–Ω–¥–µ–Ω—Ç|–†–µ–¥–∞–∫—Ü–∏—è|–ò—Å—Ç–æ—á–Ω–∏–∫|–§–æ—Ç–æ|–ò–ª–ª—é—Å—Ç—Ä–∞—Ü–∏—è)\s*:\s*[^\n]+',
        r'–ß–∏—Ç–∞–π—Ç–µ —Ç–∞–∫–∂–µ[^\n]*',
        r'–ü–æ–¥–ø–∏—Å(—ã–≤–∞–π—Ç–µ—Å—å|–∫–∞)[^\n]*',
        r'–ú–∞—Ç–µ—Ä–∏–∞–ª.*–ø–∞—Ä—Ç–Ω–µ—Ä–æ–≤[^\n]*',
        r'–†–µ–∫–ª–∞–º–∞[^\n]*',
        r'–ö–æ–º–º–µ–Ω—Ç–∞—Ä(–∏–π|–∏–∏)[^\n]*',
        r'–ú—ã –≤ —Å–æ—Ü—Å–µ—Ç—è—Ö[^\n]*',
        r'–ü—Ä–∏—Å–ª–∞—Ç—å –Ω–æ–≤–æ—Å—Ç—å[^\n]*',
        r'–û–±—Å—É–¥–∏—Ç—å –≤ —Ç–µ–ª–µ–≥—Ä–∞–º–µ[^\n]*',
        r'https?://\S+',
    ]
    for p in patterns:
        text = re.sub(p, ' ', text, flags=re.IGNORECASE)
    text = re.sub(r'[!?]{3,}', ' ', text)
    return re.sub(r'\s+', ' ', text).strip()

# ===================== CLASS =====================

class NewsBot:
    def __init__(self, bot_token: str, channel_id: str):
        self.bot = Bot(token=bot_token)
        self.channel_id = channel_id
        self.session: Optional[aiohttp.ClientSession] = None
        self.posted_hashes: Set[str] = set()
        self.failed_sources: Set[str] = set()
        self.source_priority: Dict[str, int] = {}
        self.deleted_posts_tracker: Dict[str, datetime] = {}
        self.last_publication_time: Optional[datetime] = None

        self.recent_sources: deque[str] = deque(maxlen=15)

        self.load_hashes()
        self.load_source_stats()
        self.load_recent_sources()

    # ---------- persistence ----------

    def load_hashes(self):
        try:
            if os.path.exists('posted_hashes.txt'):
                with open('posted_hashes.txt', 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                    recent_lines = lines[-1000:] if len(lines) > 1000 else lines
                    self.posted_hashes = set(line.strip() for line in recent_lines if line.strip())
                logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.posted_hashes)} —Ö–µ—à–µ–π.")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ö–µ—à–µ–π: {e}")

    def load_source_stats(self):
        try:
            if os.path.exists('source_stats.json'):
                with open('source_stats.json', 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.source_priority = data.get('priority', {})
                    deleted_data = data.get('deleted', {})
                    self.deleted_posts_tracker = {
                        source: datetime.fromisoformat(date_str)
                        for source, date_str in deleted_data.items()
                    }
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {e}")

    def save_source_stats(self):
        try:
            data = {
                'priority': self.source_priority,
                'deleted': {
                    source: date.isoformat()
                    for source, date in self.deleted_posts_tracker.items()
                }
            }
            with open('source_stats.json', 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {e}")

    def load_recent_sources(self):
        try:
            if os.path.exists('recent_sources.json'):
                with open('recent_sources.json', 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    items = data.get('recent', [])
                    self.recent_sources = deque(items, maxlen=self.recent_sources.maxlen)
                logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ –Ω–µ–¥–∞–≤–Ω–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {len(self.recent_sources)}")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ recent_sources: {e}")

    def save_recent_sources(self):
        try:
            with open('recent_sources.json', 'w', encoding='utf-8') as f:
                json.dump({'recent': list(self.recent_sources)}, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ recent_sources: {e}")

    # ---------- duplicates ----------

    def _hash_pair(self, url: str, title: str) -> str:
        u = canon_url(url)
        t = normalize_title(title).lower()
        return hashlib.md5((u + '|' + t).encode('utf-8')).hexdigest()

    def save_hash(self, url: str, title: str):
        h = self._hash_pair(url, title)
        if h not in self.posted_hashes:
            self.posted_hashes.add(h)
            try:
                with open('posted_hashes.txt', 'a', encoding='utf-8') as f:
                    f.write(h + '\n')
            except Exception as e:
                logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ö–µ—à: {e}")

    def is_duplicate(self, url: str, title: str) -> bool:
        return self._hash_pair(url, title) in self.posted_hashes

    # ---------- quality ----------

    def calculate_finance_score(self, title: str, content: str) -> int:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –±–∞–ª–ª—ã —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π —Ç–µ–º–∞—Ç–∏–∫–∏ (0-10+)"""
        text = f"{title} {content}".lower()
        
        for pattern in EXCLUDE_PATTERNS:
            if re.search(pattern, text, re.IGNORECASE):
                return 0
        
        score = 0
        for kw in FINANCE_KEYWORDS:
            if kw in text:
                if kw in ['–±–∞–Ω–∫', '–∫—Ä–µ–¥–∏—Ç', '–∏–ø–æ—Ç–µ–∫–∞', '—Å—Ç–∞–≤–∫–∞', '—Ü–±', '–∏–Ω—Ñ–ª—è—Ü–∏—è']:
                    score += 2
                else:
                    score += 1
        
        return score

    def is_finance_related(self, title: str, content: str) -> bool:
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π —Ç–µ–º–∞—Ç–∏–∫–∏"""
        score = self.calculate_finance_score(title, content)
        return score >= 3

    @staticmethod
    def clean_text(text: str) -> str:
        if not text:
            return ""
        soup = BeautifulSoup(text, "html.parser")
        for elem in soup.find_all(['script', 'style', 'nav', 'header', 'footer', 'aside', 'advertisement', 'iframe', 'form']):
            elem.decompose()
        for clip in soup.find_all(class_=lambda x: x and any(w in str(x).lower() for w in ['clip', 'ad', 'banner', 'promo', 'recommended', 'social', 'share'])):
            clip.decompose()
        for ul in soup.find_all('ul'):
            t = ul.get_text(" ")
            if any(k in t.lower() for k in ['–±–∞–Ω–∫', '–≤–∫–ª–∞–¥', '–∫—Ä–µ–¥–∏—Ç', '–∫–∞—Ä—Ç–∞', '–∏–ø–æ—Ç–µ–∫–∞', '—Ä–µ–∫–ª–∞–º']):
                ul.decompose()
        txt = soup.get_text(" ")
        txt = html.unescape(txt)
        txt = strip_byline_dates_everywhere(txt)
        return txt

    def extract_hashtags(self, title: str, content: str) -> List[str]:
        text = f"{title} {content}".lower()
        hashtags = set()
        for keyword, tags in KEYWORDS_TO_HASHTAGS.items():
            if keyword in text:
                hashtags.update(tags)
        if any(w in text for w in ["–±–∏—Ä–∂–∞", "—Ç—Ä–µ–π–¥–∏–Ω–≥", "–∏–Ω–≤–µ—Å—Ç"]):
            hashtags.add("#–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏")
        if any(w in text for w in ["–∫—Ä–∏–ø—Ç–æ", "–±–∏—Ç–∫–æ–∏–Ω", "–±–ª–æ–∫—á–µ–π–Ω", "–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞"]):
            hashtags.add("#–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç—ã")
        if any(w in text for w in ["–Ω–µ—Ñ—Ç—å", "–≥–∞–∑", "—ç–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞"]):
            hashtags.add("#—ç–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞")
        if any(w in text for w in ["—Å–∞–Ω–∫—Ü–∏–∏", "—ç–º–±–∞—Ä–≥–æ", "–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è"]):
            hashtags.add("#–º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ–û—Ç–Ω–æ—à–µ–Ω–∏—è")
        return sorted(hashtags)[:5]

    def get_relevant_emoji(self, title: str, content: str) -> str:
        text = f"{title} {content}".lower()
        for keyword, emoji in sorted(TOPIC_TO_EMOJI.items(), key=lambda x: len(x[0]), reverse=True):
            if keyword in text:
                return emoji
        return "üì∞"

    async def fetch_full_article_text(self, url: str) -> str:
        """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Å —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–º–∏ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º–∏"""
        max_retries = 2  # –£–º–µ–Ω—å—à–∏–ª–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫
        u = canon_url(url)
        
        for attempt in range(max_retries):
            try:
                headers = HEADERS.copy()
                headers['User-Agent'] = random.choice(USER_AGENTS)
                async with self.session.get(u, headers=headers, timeout=10) as resp:  # –£–º–µ–Ω—å—à–∏–ª–∏ —Ç–∞–π–º–∞—É—Ç
                    if resp.status != 200:
                        if attempt < max_retries - 1:
                            await asyncio.sleep(2 ** attempt)
                        continue
                    html_text = await resp.text()
                    soup = BeautifulSoup(html_text, "html.parser")
                    
                    # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
                    for elem in soup.find_all(['script', 'style', 'nav', 'header', 'footer', 'aside', 'advertisement', 'iframe', 'form']):
                        elem.decompose()
                    
                    for ad in soup.find_all(class_=lambda x: x and any(w in str(x).lower() for w in ['ad', 'banner', 'promo', 'recommended', 'social', 'share'])):
                        ad.decompose()

                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
                    content = None
                    for sel in UNIVERSAL_SELECTORS:
                        try:
                            if sel.startswith('.'):
                                content = soup.find(class_=sel[1:])
                            else:
                                content = soup.find(sel)
                            if content:
                                break
                        except Exception:
                            continue
                    
                    if not content:
                        all_text = soup.get_text(" ")
                        return self.clean_text(all_text)
                    
                    return self.clean_text(content.get_text(" "))

            except (asyncio.TimeoutError, aiohttp.ClientError):
                if attempt < max_retries - 1:
                    await asyncio.sleep(2 ** attempt)
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {u}: {e}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(2 ** attempt)
        
        return ""

    def smart_truncate(self, text: str, max_length: int = MAX_CONTENT_LENGTH) -> str:
        if len(text) <= max_length:
            return text
        truncated = text[:max_length + 1]
        last_end = max(truncated.rfind('.'), truncated.rfind('!'), truncated.rfind('?'))
        if last_end > max_length - 100:
            return truncated[:last_end + 1]
        else:
            return truncated[:max_length].rstrip() + "‚Ä¶"

    def format_message(self, title: str, content: str, url: str) -> str:
        title = normalize_title(title)
        full_text = self.clean_text(content)
        if full_text.startswith(title):
            full_text = full_text[len(title):].lstrip(":.- ")
        truncated = self.smart_truncate(full_text, MAX_CONTENT_LENGTH)

        # –£–ø—Ä–æ—â–µ–Ω–Ω–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–±–∑–∞—Ü–µ–≤
        sentences = re.split(r'(?<=[.!?])\s+', truncated)
        paragraphs = []
        current_para = ""
        
        for sentence in sentences:
            if not current_para:
                current_para = sentence
            elif len(current_para + " " + sentence) < 150:  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã –∞–±–∑–∞—Ü–∞
                current_para += " " + sentence
            else:
                if len(current_para) > 30:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∞–±–∑–∞—Ü–∞
                    paragraphs.append(current_para.strip())
                current_para = sentence
        
        if current_para and len(current_para) > 30:
            paragraphs.append(current_para.strip())

        formatted = "\n\n".join(paragraphs)
        hashtags = self.extract_hashtags(title, content)
        hashtag_line = "\n\n" + " ".join(hashtags) if hashtags else ""
        emoji = self.get_relevant_emoji(title, content)

        message = (
            f"<b>{emoji} {html.escape(title)}</b>\n\n"
            f"{html.escape(formatted)}\n\n"
            f"üëâ <a href='{html.escape(canon_url(url))}'>–ß–∏—Ç–∞—Ç—å –¥–∞–ª–µ–µ</a>"
            f"{hashtag_line}"
        )
        
        if len(message) > 3900:
            message = message[:3897] + "..."
        return message

    # ---------- fetching ----------

    async def fetch_feed(self, url: str) -> List[Dict]:
        max_retries = 2
        for attempt in range(max_retries):
            try:
                headers = HEADERS.copy()
                headers['User-Agent'] = random.choice(USER_AGENTS)
                async with self.session.get(url, headers=headers, timeout=10) as response:
                    if response.status != 200:
                        self.failed_sources.add(url)
                        if attempt < max_retries - 1:
                            await asyncio.sleep(2 ** attempt)
                        continue
                    
                    content = await response.text()
                    feed = await asyncio.to_thread(feedparser.parse, content)
                    entries = []
                    
                    for entry in feed.entries:
                        title = (entry.get("title") or "").strip()
                        if not title:
                            continue
                        if "–≤–∏–¥–µ–æ" in title.lower() or "video" in title.lower():
                            continue
                            
                        title = normalize_title(title)
                        link = canon_url((entry.get("link") or "").strip())
                        description = entry.get("description", "") or entry.get("summary", "") or ""
                        
                        if "<![CDATA[" in description:
                            description = re.sub(r'<!\[CDATA\[(.*?)\]\]>', r'\1', description, flags=re.DOTALL)

                        soup_desc = BeautifulSoup(description, "html.parser")
                        description = strip_byline_dates_everywhere(soup_desc.get_text(" "))

                        if title and link:
                            finance_score = self.calculate_finance_score(title, description)
                            if finance_score >= 2:
                                entries.append({
                                    "title": title,
                                    "url": link,
                                    "content": description,
                                    "source": url,
                                    "domain": domain_of(link),
                                    "finance_score": finance_score
                                })
                    
                    logger.info(f"{urlparse(url).netloc}: {len(entries)} –Ω–æ–≤–æ—Å—Ç–µ–π")
                    return entries
                    
            except (asyncio.TimeoutError, aiohttp.ClientError):
                if attempt < max_retries - 1:
                    await asyncio.sleep(2 ** attempt)
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ RSS {url}: {e}")
                self.failed_sources.add(url)
                if attempt < max_retries - 1:
                    await asyncio.sleep(2 ** attempt)
        
        return []

    # ---------- selection & rotation ----------

    def select_news_fair(self, news_items: List[Dict], k: int) -> List[Dict]:
        """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –æ—Ç–±–æ—Ä —Å —Ä–æ—Ç–∞—Ü–∏–µ–π –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        if not news_items:
            return []

        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        uniq = {}
        for n in news_items:
            key = (canon_url(n["url"]), normalize_title(n["title"]).lower())
            if key not in uniq:
                uniq[key] = n
        items = list(uniq.values())

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–∞—á–µ—Å—Ç–≤—É
        items.sort(key=lambda x: x.get("finance_score", 0), reverse=True)

        recent = set(self.recent_sources)
        result = []
        used_domains = set()

        # –°–Ω–∞—á–∞–ª–∞ –±–µ—Ä–µ–º –∏–∑ –Ω–æ–≤—ã—Ö –¥–æ–º–µ–Ω–æ–≤
        for item in items:
            if len(result) >= k:
                break
            if item["domain"] not in recent and item["domain"] not in used_domains:
                result.append(item)
                used_domains.add(item["domain"])

        # –ó–∞—Ç–µ–º –¥–æ–±–∏—Ä–∞–µ–º –∏–∑ –æ—Å—Ç–∞–ª—å–Ω—ã—Ö
        for item in items:
            if len(result) >= k:
                break
            if item not in result and (not result or result[-1]["domain"] != item["domain"]):
                result.append(item)
                used_domains.add(item["domain"])

        # –û–±–Ω–æ–≤–ª—è–µ–º –∏—Å—Ç–æ—Ä–∏—é –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        for item in result:
            self.recent_sources.append(item["domain"])
        self.save_recent_sources()

        logger.info(f"–û—Ç–æ–±—Ä–∞–Ω–æ {len(result)} –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ {len(used_domains)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
        return result[:k]

    # ---------- publish ----------

    async def publish_post(self, title: str, content: str, url: str, source: str = "") -> bool:
        if self.is_duplicate(url, title):
            logger.info(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ (–¥—É–±–ª–∏–∫–∞—Ç): {title[:60]}...")
            return False
        
        if not self.is_finance_related(title, content):
            logger.info(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ (–Ω–µ —Ñ–∏–Ω—Ç–µ–º–∞—Ç–∏–∫–∞): {title[:60]}...")
            return False

        full_text = await self.fetch_full_article_text(url)
        use_text = full_text if full_text.strip() else content
        cleaned = self.clean_text(use_text)
        
        if len(cleaned) < MIN_CONTENT_LENGTH:
            logger.info(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ (–º–∞–ª–æ —Ç–µ–∫—Å—Ç–∞ {len(cleaned)} < {MIN_CONTENT_LENGTH}): {title[:60]}...")
            return False

        max_retries = 2
        for attempt in range(max_retries):
            try:
                message = self.format_message(title, use_text, url)
                await self.bot.send_message(
                    chat_id=self.channel_id,
                    text=message,
                    parse_mode='HTML',
                    disable_web_page_preview=True
                )
                logger.info(f"‚úÖ –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ: {title[:60]}...")
                self.save_hash(url, title)
                if source:
                    self.source_priority[source] = self.source_priority.get(source, 0) + 1
                    self.save_source_stats()
                return True
                
            except error.RetryAfter as e:
                logger.warning(f"Rate limit, –∂–¥—ë–º {e.retry_after} —Å–µ–∫...")
                await asyncio.sleep(e.retry_after)
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ '{title}': {e}")
                if attempt == max_retries - 1:
                    if source:
                        self.deleted_posts_tracker[source] = datetime.now()
                        self.source_priority[source] = self.source_priority.get(source, 0) - 1
                        self.save_source_stats()
                    return False
                await asyncio.sleep(2 ** attempt)
        
        return False

    # ---------- scheduling ----------

    def generate_post_schedule(self) -> List[datetime]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç 5 —Å–ª—É—á–∞–π–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω —Å 8:00 –¥–æ 20:00 –ø–æ –ú–°–ö"""
        try:
            msk = pytz.timezone('Europe/Moscow')
            now = datetime.now(msk)
            
            start_hour, end_hour = 8, 20
            base_date = now.replace(hour=start_hour, minute=0, second=0, microsecond=0)
            
            if now.hour >= end_hour:
                base_date = (now + timedelta(days=1)).replace(hour=start_hour, minute=0, second=0, microsecond=0)

            times = []
            for _ in range(MAX_POSTS_PER_DAY):
                # –°–ª—É—á–∞–π–Ω–æ–µ –≤—Ä–µ–º—è –≤ —Ä–∞–±–æ—á–µ–º –∏–Ω—Ç–µ—Ä–≤–∞–ª–µ
                random_hour = random.randint(start_hour, end_hour - 1)
                random_minute = random.randint(0, 59)
                pub_time = base_date.replace(hour=random_hour, minute=random_minute)
                times.append(pub_time)
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º –±—É–¥—É—â–∏–µ –≤—Ä–µ–º–µ–Ω–∞
            times.sort()
            future_times = [t for t in times if t > now]
            
            # –ï—Å–ª–∏ –Ω—É–∂–Ω–æ –±–æ–ª—å—à–µ –≤—Ä–µ–º–µ–Ω - –¥–æ–±–∞–≤–ª—è–µ–º
            while len(future_times) < MAX_POSTS_PER_DAY:
                extra_minutes = random.randint(30, 180)
                extra_time = now + timedelta(minutes=extra_minutes)
                if extra_time.hour < end_hour:
                    future_times.append(extra_time)
            
            return sorted(future_times)[:MAX_POSTS_PER_DAY]
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è: {e}")
            # –ü—Ä–æ—Å—Ç–æ–π fallback
            msk = pytz.timezone('Europe/Moscow')
            base_time = datetime.now(msk)
            return [base_time + timedelta(minutes=30 * i) for i in range(MAX_POSTS_PER_DAY)]

    # ---------- main ----------

    async def run(self):
        connector = aiohttp.TCPConnector(limit=8, ttl_dns_cache=300)  # –£–º–µ–Ω—å—à–∏–ª–∏ –ª–∏–º–∏—Ç
        timeout = aiohttp.ClientTimeout(total=15)

        async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
            self.session = session

            # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π —Å–±–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π
            async def fetch_single_source(url):
                if url in self.failed_sources:
                    return []
                try:
                    return await self.fetch_feed(url)
                except Exception:
                    self.failed_sources.add(url)
                    return []

            # –û—Å–Ω–æ–≤–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
            tasks = [fetch_single_source(src) for src in RSS_SOURCES]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            all_news = []
            for result in results:
                if isinstance(result, list):
                    all_news.extend(result)

            # –†–µ–∑–µ—Ä–≤–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if len(all_news) < MAX_POSTS_PER_DAY:
                backup_tasks = [fetch_single_source(src) for src in BACKUP_SOURCES]
                backup_results = await asyncio.gather(*backup_tasks, return_exceptions=True)
                for result in backup_results:
                    if isinstance(result, list):
                        all_news.extend(result)

            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è
            filtered = []
            seen_urls = set()
            for item in all_news:
                url_c = canon_url(item["url"])
                if (url_c not in seen_urls and 
                    not self.is_duplicate(item["url"], item["title"]) and
                    self.is_finance_related(item["title"], item["content"]) and
                    len(self.clean_text(item["content"])) >= MIN_CONTENT_LENGTH):
                    filtered.append(item)
                    seen_urls.add(url_c)

            if not filtered:
                logger.info("–ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π.")
                return

            logger.info(f"–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(filtered)} –Ω–æ–≤–æ—Å—Ç–µ–π")

            # –û—Ç–±–æ—Ä –∏ —Ä–æ—Ç–∞—Ü–∏—è
            final_news = self.select_news_fair(filtered, MAX_POSTS_PER_DAY)

            if not final_news:
                logger.info("–ù–µ—á–µ–≥–æ –ø—É–±–ª–∏–∫–æ–≤–∞—Ç—å –ø–æ—Å–ª–µ —Ä–æ—Ç–∞—Ü–∏–∏.")
                return

            schedule = self.generate_post_schedule()
            logger.info(f"–†–∞—Å–ø–∏—Å–∞–Ω–∏–µ –Ω–∞ {len(schedule)} –ø—É–±–ª–∏–∫–∞—Ü–∏–π:")
            for i, t in enumerate(schedule, 1):
                logger.info(f"  {i}. {t.strftime('%H:%M')} –ú–°–ö")

            # –ü—É–±–ª–∏–∫–∞—Ü–∏—è –ø–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—é
            for i, (news_item, pub_time) in enumerate(zip(final_news, schedule)):
                msk = pytz.timezone('Europe/Moscow')
                now = datetime.now(msk)
                
                if pub_time > now:
                    wait_seconds = (pub_time - now).total_seconds()
                    logger.info(f"–û–∂–∏–¥–∞–Ω–∏–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ {i+1}: {int(wait_seconds)} —Å–µ–∫")
                    await asyncio.sleep(wait_seconds)

                success = await self.publish_post(
                    title=news_item["title"],
                    content=news_item["content"],
                    url=news_item["url"],
                    source=news_item["source"]
                )
                
                self.recent_sources.append(news_item["domain"])
                self.save_recent_sources()

                if i < len(final_news) - 1:
                    await asyncio.sleep(random.uniform(3, 8))

            logger.info("‚úÖ –¶–∏–∫–ª –ø—É–±–ª–∏–∫–∞—Ü–∏–π –∑–∞–≤–µ—Ä—à—ë–Ω.")


async def main():
    try:
        bot = NewsBot(BOT_TOKEN, CHANNEL_ID)
        await bot.run()
    except KeyboardInterrupt:
        logger.info("–ë–æ—Ç –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        raise


if __name__ == "__main__":
    asyncio.run(main())