import asyncio
import hashlib
import html
import json
import logging
import os
import random
import re
from collections import deque
from datetime import datetime, timedelta
from typing import List, Dict, Set, Optional, Tuple
from urllib.parse import urlparse, urlunparse, parse_qsl, urlencode

import aiohttp
import feedparser
import pytz
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from telegram import Bot, error

# ===================== ENV & LOG =====================

load_dotenv()

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    handlers=[
        logging.FileHandler('bot.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)
logging.getLogger("httpx").setLevel(logging.WARNING)

# ===================== CONST =====================

RSS_SOURCES = [
    "https://www.finam.ru/analytics/rsspoint/",
    "https://www.cbr.ru/rss/eventrss",
    "https://www.vedomosti.ru/rss/rubric/finance/banks.xml",
    "https://arb.ru/rss/news/",
    "https://www.bfm.ru/news.rss?rubric=28",
    "https://ria.ru/export/rss2/archive/index.xml",
    "https://www.vestifinance.ru/rss/news",
]

BACKUP_SOURCES = [
    "https://www.interfax.ru/rss.asp",
]

BOT_TOKEN = os.getenv("BOT_TOKEN")
CHANNEL_ID = os.getenv("CHANNEL_ID")

if not BOT_TOKEN:
    raise ValueError("‚ùå BOT_TOKEN –Ω–µ –∑–∞–¥–∞–Ω –≤ .env —Ñ–∞–π–ª–µ!")
if not CHANNEL_ID:
    raise ValueError("‚ùå CHANNEL_ID –Ω–µ –∑–∞–¥–∞–Ω –≤ .env —Ñ–∞–π–ª–µ!")

if CHANNEL_ID.lstrip('-').isdigit() and len(CHANNEL_ID.lstrip('-')) >= 10 and not CHANNEL_ID.startswith('-100'):
    CHANNEL_ID = '-100' + CHANNEL_ID.lstrip('-')

MAX_POSTS_PER_DAY = 3
MAX_CONTENT_LENGTH = 800
MIN_CONTENT_LENGTH = 50

USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 12_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.1 Safari/605.1.15',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0 Safari/537.36',
]

HEADERS = {'User-Agent': random.choice(USER_AGENTS)}

FINANCE_KEYWORDS = [
    '–±–∞–Ω–∫', '–∫—Ä–µ–¥–∏—Ç', '–∏–ø–æ—Ç–µ–∫–∞', '–≤–∫–ª–∞–¥', '–¥–µ–ø–æ–∑–∏—Ç', '–∞–∫—Ü–∏—è', '–æ–±–ª–∏–≥–∞—Ü–∏—è',
    '—Ä—É–±–ª—å', '–¥–æ–ª–ª–∞—Ä', '–µ–≤—Ä–æ', '–∏–Ω—Ñ–ª—è—Ü–∏—è', '—Å—Ç–∞–≤–∫–∞', '—Ü–±', '—Ñ—Ä—Å', '–±–∏—Ä–∂–∞',
    '–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞', '–Ω–µ—Ñ—Ç—å', '–≥–∞–∑', '—ç–∫–æ–Ω–æ–º–∏–∫–∞', '—Ä—ã–Ω–æ–∫', '–∏–Ω–≤–µ—Å—Ç', '—Ñ–∏–Ω–∞–Ω—Å',
    '–ª–∏–∫–≤–∏–¥–Ω–æ—Å—Ç—å', '–¥–∏–≤–∏–¥–µ–Ω–¥', '–∫—Ä–∏–∑–∏—Å', '—Å–∞–Ω–∫—Ü–∏–∏', '—Ä–µ–≥—É–ª—è—Ç–æ—Ä', '—Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π –±–∞–Ω–∫',
    '–∫—Ä–µ–¥–∏—Ç–Ω–∞—è', '–∑–∞–µ–º', '–∑–∞–π–º', '—Ä–µ—Ñ–∏–Ω–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–∏–µ', '—Å–±–µ—Ä–µ–∂–µ–Ω–∏—è', '—Ñ–æ–Ω–¥–æ–≤—ã–π',
    '–≤–∞–ª—é—Ç–Ω—ã–π', '–∫—É—Ä—Å', '–æ–±–º–µ–Ω', '–ø–ª–∞—Ç–µ–∂', '–ø–µ—Ä–µ–≤–æ–¥', '–∫–∞—Ä—Ç–∞', '–±—Ä–æ–∫–µ—Ä',
    '–∫–æ—Ç–∏—Ä–æ–≤–∫–∏', '–∏–Ω–¥–µ–∫—Å', '–∫–∞–ø–∏—Ç–∞–ª–∏–∑–∞—Ü–∏—è', '–∞–∫—Ç–∏–≤', '–ø–∞—Å—Å–∏–≤', '–±–∞–ª–∞–Ω—Å', '–æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å',
    '–ø—Ä–∏–±—ã–ª—å', '—É–±—ã—Ç–æ–∫', '–≤—ã–ø–ª–∞—Ç—ã', '–∫–≤–∞—Ä—Ç–∞–ª—å–Ω—ã–π', '–≥–æ–¥–æ–≤–æ–π', '–Ω–∞–¥–∑–æ—Ä', '–ª–∏—Ü–µ–Ω–∑–∏—è',
    '—Å–∞–Ω–∞—Ü–∏—è', '–±–∞–Ω–∫—Ä–æ—Ç—Å—Ç–≤–æ', '—Ñ–æ—Ä–µ–∫—Å', '–∏–Ω–≤–µ—Å—Ç–æ—Ä', '–ø–æ—Ä—Ç—Ñ–µ–ª—å', '—Ä–∏—Å–∫', '–¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å',
    '–ø—Ä–æ—Ü–µ–Ω—Ç', '–∫–ª—é—á–µ–≤–∞—è —Å—Ç–∞–≤–∫–∞', '–º–æ–Ω–µ—Ç–∞—Ä–Ω—ã–π', '—Ñ–∏—Å–∫–∞–ª—å–Ω—ã–π', '–±—é–¥–∂–µ—Ç', '–Ω–∞–ª–æ–≥', '—Ç–∞—Ä–∏—Ñ',
    '—Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ', '–ø–µ–Ω—Å–∏–æ–Ω–Ω—ã–π', '–ª–∏–∑–∏–Ω–≥', '—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥'
]

KEYWORDS_TO_HASHTAGS = {
    "–±–∞–Ω–∫": ["#–±–∞–Ω–∫–∏", "#—Ñ–∏–Ω–∞–Ω—Å—ã", "#–±–∞–Ω–∫–æ–≤—Å–∫–∏–π–°–µ–∫—Ç–æ—Ä"],
    "–∫—Ä–µ–¥–∏—Ç": ["#–∫—Ä–µ–¥–∏—Ç", "#–∫—Ä–µ–¥–∏—Ç–æ–≤–∞–Ω–∏–µ", "#–∑–∞–π–º—ã"],
    "–∏–ø–æ—Ç–µ–∫–∞": ["#–∏–ø–æ—Ç–µ–∫–∞", "#–Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å", "#–∂–∏–ª—å–µ"],
    "–≤–∫–ª–∞–¥": ["#–≤–∫–ª–∞–¥—ã", "#–¥–µ–ø–æ–∑–∏—Ç—ã", "#—Å–±–µ—Ä–µ–∂–µ–Ω–∏—è"],
    "–∞–∫—Ü–∏—è": ["#–∞–∫—Ü–∏–∏", "#–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏", "#—Ñ–æ–Ω–¥–æ–≤—ã–π–†—ã–Ω–æ–∫"],
    "–æ–±–ª–∏–≥–∞—Ü–∏—è": ["#–æ–±–ª–∏–≥–∞—Ü–∏–∏", "#–±–æ–Ω–¥—Å", "#–¥–æ–ª–≥–æ–≤—ã–µ–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã"],
    "—Ä—É–±–ª—å": ["#—Ä—É–±–ª—å", "#–≤–∞–ª—é—Ç–∞", "#–∫—É—Ä—Å–†—É–±–ª—è"],
    "–¥–æ–ª–ª–∞—Ä": ["#–¥–æ–ª–ª–∞—Ä", "#USD", "#–≤–∞–ª—é—Ç–∞"],
    "–µ–≤—Ä–æ": ["#–µ–≤—Ä–æ", "#EUR", "#–≤–∞–ª—é—Ç–∞"],
    "–∏–Ω—Ñ–ª—è—Ü–∏—è": ["#–∏–Ω—Ñ–ª—è—Ü–∏—è", "#—Ü–µ–Ω—ã", "#—ç–∫–æ–Ω–æ–º–∏–∫–∞"],
    "–∫–ª—é—á–µ–≤–∞—è —Å—Ç–∞–≤–∫–∞": ["#–∫–ª—é—á–µ–≤–∞—è–°—Ç–∞–≤–∫–∞", "#–¶–ë", "#–ø—Ä–æ—Ü–µ–Ω—Ç–Ω–∞—è–°—Ç–∞–≤–∫–∞"],
    "—Ü–±": ["#–¶–µ–Ω—Ç—Ä–æ–±–∞–Ω–∫", "#—Ä–µ–≥—É–ª—è—Ç–æ—Ä", "#–±–∞–Ω–∫–†–æ—Å—Å–∏–∏"],
    "—Ñ—Ä—Å": ["#–§–†–°", "#–°–®–ê"],
    "–±–∏—Ä–∂–∞": ["#–±–∏—Ä–∂–∞", "#—Ç—Ä–µ–π–¥–∏–Ω–≥", "#—Ñ–æ–Ω–¥–æ–≤—ã–π–†—ã–Ω–æ–∫"],
    "–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞": ["#–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞", "#–±–∏—Ç–∫–æ–∏–Ω", "#–±–ª–æ–∫—á–µ–π–Ω"],
    "–Ω–µ—Ñ—Ç—å": ["#–Ω–µ—Ñ—Ç—å", "#—ç–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞"],
    "–≥–∞–∑": ["#–≥–∞–∑", "#—ç–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞"],
    "—ç–∫–æ–Ω–æ–º–∏–∫–∞": ["#—ç–∫–æ–Ω–æ–º–∏–∫–∞", "#–º–∞–∫—Ä–æ—ç–∫–æ–Ω–æ–º–∏–∫–∞"],
}

TOPIC_TO_EMOJI = {
    "–±–∞–Ω–∫": "üè¶", "–∫—Ä–µ–¥–∏—Ç": "üí≥", "–∏–ø–æ—Ç–µ–∫–∞": "üè†", "–≤–∫–ª–∞–¥": "üí∞",
    "–∞–∫—Ü–∏—è": "üìà", "–æ–±–ª–∏–≥–∞—Ü–∏—è": "üìä", "—Ä—É–±–ª—å": "‚ÇΩ", "–¥–æ–ª–ª–∞—Ä": "üíµ", "–µ–≤—Ä–æ": "üí∂",
    "–∏–Ω—Ñ–ª—è—Ü–∏—è": "üìâ", "–∫–ª—é—á–µ–≤–∞—è —Å—Ç–∞–≤–∫–∞": "üìå", "—Ü–±": "üá∑üá∫", "—Ñ—Ä—Å": "üá∫üá∏",
    "–±–∏—Ä–∂–∞": "üìä", "–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞": "‚Çø", "–Ω–µ—Ñ—Ç—å": "üõ¢Ô∏è", "–≥–∞–∑": "üî•",
    "—ç–∫–æ–Ω–æ–º–∏–∫–∞": "üåê", "—Ä—ã–Ω–æ–∫": "ü§ù", "–∏–Ω–≤–µ—Å—Ç": "üíº", "–ª–∏–∫–≤–∏–¥–Ω–æ—Å—Ç—å": "üíß",
    "–¥–∏–≤–∏–¥–µ–Ω–¥": "üéÅ", "–∫—Ä–∏–∑–∏—Å": "‚ö†Ô∏è", "—Å–∞–Ω–∫—Ü–∏–∏": "üö´", "—Ä–µ–≥—É–ª—è—Ç–æ—Ä": "üëÆ",
}

RUS_MONTHS = r'(—è–Ω–≤–∞—Ä[—å—è]|—Ñ–µ–≤—Ä–∞–ª[—å—è]|–º–∞—Ä—Ç[–∞–µ]?|–∞–ø—Ä–µ–ª[—å—è]|–º–∞[–µ—è]|–∏—é–Ω[—å—è]|–∏—é–ª[—å—è]|–∞–≤–≥—É—Å—Ç[–∞–µ]?|—Å–µ–Ω—Ç—è–±—Ä[—å—è]|–æ–∫—Ç—è–±—Ä[—å—è]|–Ω–æ—è–±—Ä[—å—è]|–¥–µ–∫–∞–±—Ä[—å—è])'

# ===================== UTILS =====================

def canon_url(url: str) -> str:
    """–£–¥–∞–ª—è–µ–º UTM –∏ –ø—Ä–æ—á–∏–µ –º—É—Å–æ—Ä–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Å—Å—ã–ª–∫—É."""
    try:
        u = urlparse(url)
        q = [(k, v) for k, v in parse_qsl(u.query, keep_blank_values=True)
             if not k.lower().startswith('utm')
             and k.lower() not in {'fbclid', 'gclid', 'yclid', 'utm_referrer'}]
        return urlunparse((u.scheme, u.netloc, u.path, u.params, urlencode(q, doseq=True), ''))
    except Exception:
        return url

def domain_of(url: str) -> str:
    try:
        return urlparse(url).netloc.lower()
    except Exception:
        return url

def normalize_title(title: str) -> str:
    """–£–±–∏—Ä–∞–µ–º –¥–∞—Ç—ã/—Ö–≤–æ—Å—Ç—ã –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞."""
    if not title:
        return ""
    # 12 –º–∞—è 2025, 12 –º–∞—è 2025 –≥., 12.05.2025 –∏ —Ç.–ø.
    title = re.sub(rf'\b\d{{1,2}}\s+{RUS_MONTHS}\s+\d{{4}}\s*–≥?\.?,?\s*', ' ', title, flags=re.IGNORECASE)
    title = re.sub(r'\b\d{1,2}[./-]\d{1,2}[./-]\d{2,4}\b', ' ', title)
    # –•–≤–æ—Å—Ç—ã –≤–∏–¥–∞ "‚Äî –í–µ–¥–æ–º–æ—Å—Ç–∏", "- –†–ò–ê –ù–æ–≤–æ—Å—Ç–∏"
    title = re.sub(r'\s*[-‚Äî‚Äì]\s*[^\n]+$', '', title).strip()
    return re.sub(r'\s+', ' ', title).strip()

def strip_byline_dates_everywhere(text: str) -> str:
    """–£–±–∏—Ä–∞–µ–º –∞–≤—Ç–æ—Ä–æ–≤/–¥–∞—Ç—ã/—Å–ª—É–∂–µ–±–Ω—ã–µ –≤—Å—Ç–∞–≤–∫–∏ –≤ –ª—é–±–æ–º –º–µ—Å—Ç–µ —Ç–µ–∫—Å—Ç–∞."""
    if not text:
        return ""
    patterns = [
        rf'\b\d{{1,2}}\s+{RUS_MONTHS}\s+\d{{4}}\b',      # 12 –º–∞—è 2025
        r'\b\d{1,2}[:.]\d{2}\b',                         # 12:34
        r'\b\d{1,2}[./-]\d{1,2}[./-]\d{2,4}\b',         # 12.05.2025
        r'(?:–ê–≤—Ç–æ—Ä|–ö–æ—Ä—Ä–µ—Å–ø–æ–Ω–¥–µ–Ω—Ç|–†–µ–¥–∞–∫—Ü–∏—è|–ò—Å—Ç–æ—á–Ω–∏–∫|–§–æ—Ç–æ|–ò–ª–ª—é—Å—Ç—Ä–∞—Ü–∏—è)\s*:\s*[^\n]+',
        r'–ß–∏—Ç–∞–π—Ç–µ —Ç–∞–∫–∂–µ[^\n]*',
        r'–ü–æ–¥–ø–∏—Å(—ã–≤–∞–π—Ç–µ—Å—å|–∫–∞)[^\n]*',
        r'–ú–∞—Ç–µ—Ä–∏–∞–ª.*–ø–∞—Ä—Ç–Ω–µ—Ä–æ–≤[^\n]*',
        r'–†–µ–∫–ª–∞–º–∞[^\n]*',
        r'–ö–æ–º–º–µ–Ω—Ç–∞—Ä(–∏–π|–∏–∏)[^\n]*',
        r'–ú—ã –≤ —Å–æ—Ü—Å–µ—Ç—è—Ö[^\n]*',
        r'–ü—Ä–∏—Å–ª–∞—Ç—å –Ω–æ–≤–æ—Å—Ç—å[^\n]*',
        r'–û–±—Å—É–¥–∏—Ç—å –≤ —Ç–µ–ª–µ–≥—Ä–∞–º–µ[^\n]*',
        r'https?://\S+',
    ]
    for p in patterns:
        text = re.sub(p, ' ', text, flags=re.IGNORECASE)
    text = re.sub(r'[!?]{3,}', ' ', text)
    return re.sub(r'\s+', ' ', text).strip()

# ===================== CLASS =====================

class NewsBot:
    def __init__(self, bot_token: str, channel_id: str):
        self.bot = Bot(token=bot_token)
        self.channel_id = channel_id
        self.session: Optional[aiohttp.ClientSession] = None
        self.posted_hashes: Set[str] = set()
        self.failed_sources: Set[str] = set()
        self.source_priority: Dict[str, int] = {}
        self.deleted_posts_tracker: Dict[str, datetime] = {}
        self.last_publication_time: Optional[datetime] = None

        # —Ä–æ—Ç–∞—Ü–∏—è –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º –º–µ–∂–¥—É —Ü–∏–∫–ª–∞–º–∏
        self.recent_sources: deque[str] = deque(maxlen=12)

        self.load_hashes()
        self.load_source_stats()
        self.load_recent_sources()

    # ---------- persistence ----------

    def load_hashes(self):
        try:
            if os.path.exists('posted_hashes.txt'):
                with open('posted_hashes.txt', 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                    recent_lines = lines[-1000:] if len(lines) > 1000 else lines
                    self.posted_hashes = set(line.strip() for line in recent_lines if line.strip())
                logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.posted_hashes)} —Ö–µ—à–µ–π.")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ö–µ—à–µ–π: {e}")

    def load_source_stats(self):
        try:
            if os.path.exists('source_stats.json'):
                with open('source_stats.json', 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.source_priority = data.get('priority', {})
                    deleted_data = data.get('deleted', {})
                    self.deleted_posts_tracker = {
                        source: datetime.fromisoformat(date_str)
                        for source, date_str in deleted_data.items()
                    }
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {e}")

    def save_source_stats(self):
        try:
            data = {
                'priority': self.source_priority,
                'deleted': {
                    source: date.isoformat()
                    for source, date in self.deleted_posts_tracker.items()
                }
            }
            with open('source_stats.json', 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {e}")

    def load_recent_sources(self):
        try:
            if os.path.exists('recent_sources.json'):
                with open('recent_sources.json', 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    items = data.get('recent', [])
                    self.recent_sources = deque(items, maxlen=self.recent_sources.maxlen)
                logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ –Ω–µ–¥–∞–≤–Ω–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {len(self.recent_sources)}")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ recent_sources: {e}")

    def save_recent_sources(self):
        try:
            with open('recent_sources.json', 'w', encoding='utf-8') as f:
                json.dump({'recent': list(self.recent_sources)}, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ recent_sources: {e}")

    # ---------- duplicates ----------

    def _hash_pair(self, url: str, title: str) -> str:
        u = canon_url(url)
        t = normalize_title(title).lower()
        return hashlib.md5((u + '|' + t).encode('utf-8')).hexdigest()

    def save_hash(self, url: str, title: str):
        h = self._hash_pair(url, title)
        if h not in self.posted_hashes:
            self.posted_hashes.add(h)
            try:
                with open('posted_hashes.txt', 'a', encoding='utf-8') as f:
                    f.write(h + '\n')
            except Exception as e:
                logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ö–µ—à: {e}")

    def is_duplicate(self, url: str, title: str) -> bool:
        return self._hash_pair(url, title) in self.posted_hashes

    # ---------- quality ----------

    def is_finance_related(self, title: str, content: str) -> bool:
        text = f"{title} {content}".lower()
        return any(kw in text for kw in FINANCE_KEYWORDS)

    @staticmethod
    def clean_text(text: str) -> str:
        if not text:
            return ""
        soup = BeautifulSoup(text, "html.parser")
        for elem in soup.find_all(['script', 'style', 'nav', 'header', 'footer', 'aside', 'advertisement', 'iframe', 'form']):
            elem.decompose()
        for clip in soup.find_all(class_=lambda x: x and any(w in str(x).lower() for w in ['clip', 'ad', 'banner', 'promo', 'recommended', 'social', 'share'])):
            clip.decompose()
        for ul in soup.find_all('ul'):
            t = ul.get_text(" ")
            if any(k in t.lower() for k in ['–±–∞–Ω–∫', '–≤–∫–ª–∞–¥', '–∫—Ä–µ–¥–∏—Ç', '–∫–∞—Ä—Ç–∞', '–∏–ø–æ—Ç–µ–∫–∞', '—Ä–µ–∫–ª–∞–º']):
                ul.decompose()
        txt = soup.get_text(" ")
        txt = html.unescape(txt)
        txt = strip_byline_dates_everywhere(txt)
        return txt

    def extract_hashtags(self, title: str, content: str) -> List[str]:
        text = f"{title} {content}".lower()
        hashtags = set()
        for keyword, tags in KEYWORDS_TO_HASHTAGS.items():
            if keyword in text:
                hashtags.update(tags)
        if any(w in text for w in ["–±–∏—Ä–∂–∞", "—Ç—Ä–µ–π–¥–∏–Ω–≥", "–∏–Ω–≤–µ—Å—Ç"]):
            hashtags.add("#–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏")
        if any(w in text for w in ["–∫—Ä–∏–ø—Ç–æ", "–±–∏—Ç–∫–æ–∏–Ω", "–±–ª–æ–∫—á–µ–π–Ω", "–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞"]):
            hashtags.add("#–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç—ã")
        if any(w in text for w in ["–Ω–µ—Ñ—Ç—å", "–≥–∞–∑", "—ç–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞"]):
            hashtags.add("#—ç–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞")
        if any(w in text for w in ["—Å–∞–Ω–∫—Ü–∏–∏", "—ç–º–±–∞—Ä–≥–æ", "–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è"]):
            hashtags.add("#–º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ–û—Ç–Ω–æ—à–µ–Ω–∏—è")
        return sorted(hashtags)[:5]

    def get_relevant_emoji(self, title: str, content: str) -> str:
        text = f"{title} {content}".lower()
        for keyword, emoji in sorted(TOPIC_TO_EMOJI.items(), key=lambda x: len(x[0]), reverse=True):
            if keyword in text:
                return emoji
        return "üì∞"

    async def fetch_full_article_text(self, url: str) -> str:
        max_retries = 3
        u = canon_url(url)
        for attempt in range(max_retries):
            try:
                headers = HEADERS.copy()
                headers['User-Agent'] = random.choice(USER_AGENTS)
                async with self.session.get(u, headers=headers, timeout=15) as resp:
                    if resp.status != 200:
                        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {u}: {resp.status}")
                        if attempt < max_retries - 1:
                            await asyncio.sleep(2 ** attempt)
                        continue
                    html_text = await resp.text()
                    soup = BeautifulSoup(html_text, "html.parser")
                    for elem in soup.find_all(['script', 'style', 'nav', 'header', 'footer', 'aside', 'advertisement', 'iframe', 'form']):
                        elem.decompose()
                    for ad in soup.find_all(class_=lambda x: x and any(w in str(x).lower() for w in ['ad', 'banner', 'promo', 'recommended', 'social', 'share'])):
                        ad.decompose()

                    dom = domain_of(u)
                    selectors = []
                    if "finam.ru" in dom:
                        selectors = ['.article__body', '.content', 'article']
                    elif "cbr.ru" in dom:
                        selectors = ['.content', '.text', 'article']
                    elif "vedomosti.ru" in dom:
                        selectors = ['.article__body', '.article-body', 'article']
                    elif "arb.ru" in dom:
                        selectors = ['.news-detail', '.content', 'article']
                    elif "kommersant.ru" in dom or "rbc.ru" in dom or "banki.ru" in dom:
                        selectors = ['.article__text', '.article__content', '.news-text', '.article-content', 'article']
                    else:
                        selectors = ['.article-content', '.post-content', '.entry-content', '.article__body', '.article-body', 'article', '.content']

                    content = None
                    for sel in selectors:
                        try:
                            if sel.startswith('.'):
                                content = soup.find(class_=sel[1:])
                            elif sel.startswith('#'):
                                content = soup.find(id=sel[1:])
                            else:
                                content = soup.find(sel)
                            if content:
                                break
                        except Exception:
                            continue
                    if not content:
                        all_text = soup.get_text(" ")
                        return self.clean_text(all_text)
                    return self.clean_text(content.get_text(" "))

            except asyncio.TimeoutError:
                logger.warning(f"–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {u}, –ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(2 ** attempt)
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {u}: {e}, –ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(2 ** attempt)
        return ""

    def smart_truncate(self, text: str, max_length: int = MAX_CONTENT_LENGTH) -> str:
        if len(text) <= max_length:
            return text
        truncated = text[:max_length + 1]
        last_end = max(truncated.rfind('.'), truncated.rfind('!'), truncated.rfind('?'))
        if last_end > max_length - 100:
            return truncated[:last_end + 1]
        else:
            return truncated[:max_length].rstrip() + "‚Ä¶"

    def format_message(self, title: str, content: str, url: str) -> str:
        title = normalize_title(title)
        full_text = self.clean_text(content)
        if full_text.startswith(title):
            full_text = full_text[len(title):].lstrip(":.- ")
        truncated = self.smart_truncate(full_text, MAX_CONTENT_LENGTH)

        # –î–µ–ª–∏–º –Ω–∞ 1‚Äì2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤ –∞–±–∑–∞—Ü–µ
        sentences = re.split(r'(?<=[.!?])\s+', truncated)
        paragraphs, buf = [], ""
        for s in sentences:
            if not buf:
                buf = s
            elif len(re.findall(r'[.!?]', buf)) < 2:
                buf += " " + s
            else:
                paragraphs.append(buf.strip())
                buf = s
        if buf:
            paragraphs.append(buf.strip())
        formatted = "\n\n".join(p for p in paragraphs if len(p) > 20)

        hashtags = self.extract_hashtags(title, content)
        hashtag_line = "\n\n" + " ".join(hashtags) if hashtags else ""
        emoji = self.get_relevant_emoji(title, content)

        message = (
            f"<b>{emoji} {html.escape(title)}</b>\n\n"
            f"{html.escape(formatted)}\n\n"
            f"üëâ <a href='{html.escape(canon_url(url))}'>–ß–∏—Ç–∞—Ç—å –¥–∞–ª–µ–µ</a>"
            f"{hashtag_line}"
        )
        if len(message) > 3900:
            message = message[:3897] + "..."
        return message

    # ---------- fetching ----------

    async def fetch_feed(self, url: str) -> List[Dict]:
        max_retries = 3
        for attempt in range(max_retries):
            try:
                headers = HEADERS.copy()
                headers['User-Agent'] = random.choice(USER_AGENTS)
                async with self.session.get(url, headers=headers, timeout=15) as response:
                    if response.status != 200:
                        logger.warning(f"HTTP {response.status} –Ω–∞ {url}")
                        self.failed_sources.add(url)
                        if attempt < max_retries - 1:
                            await asyncio.sleep(2 ** attempt)
                        continue
                    content = await response.text()
                    feed = await asyncio.to_thread(feedparser.parse, content)
                    entries = []
                    for entry in feed.entries:
                        title = (entry.get("title") or "").strip()
                        if not title:
                            continue
                        if "–≤–∏–¥–µ–æ" in title.lower() or "video" in title.lower():
                            continue
                        title = normalize_title(title)
                        link = canon_url((entry.get("link") or "").strip())
                        description = entry.get("description", "") or entry.get("summary", "") or ""
                        if "<![CDATA[" in description:
                            description = re.sub(r'<!\[CDATA\[(.*?)\]\]>', r'\1', description, flags=re.DOTALL)

                        soup_desc = BeautifulSoup(description, "html.parser")
                        description = strip_byline_dates_everywhere(soup_desc.get_text(" "))

                        # –ê–≤—Ç–æ—Ä–æ–≤ –∏ –º–µ—Ç–∫–∏ ‚Äî –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤ –≤—ã–≤–æ–¥–µ, —á—Ç–æ–±—ã –Ω–µ ¬´–∑–∞—Å–æ—Ä—è—Ç—å¬ª
                        creator = ""

                        if title and link and self.is_finance_related(title, description):
                            entries.append({
                                "title": title,
                                "url": link,
                                "content": description,
                                "creator": creator,
                                "source": url,
                                "domain": domain_of(link),
                            })
                    logger.info(f"{urlparse(url).netloc}: {len(entries)} –Ω–æ–≤–æ—Å—Ç–µ–π")
                    return entries
            except asyncio.TimeoutError:
                logger.warning(f"–¢–∞–π–º–∞—É—Ç RSS {url}, –ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(2 ** attempt)
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ RSS {url}: {e}, –ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}")
                self.failed_sources.add(url)
                if attempt < max_retries - 1:
                    await asyncio.sleep(2 ** attempt)
        return []

    # ---------- selection & rotation ----------

    def select_news_fair(self, news_items: List[Dict], k: int) -> List[Dict]:
        """
        –°–ø—Ä–∞–≤–µ–¥–ª–∏–≤—ã–π –æ—Ç–±–æ—Ä –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º/–¥–æ–º–µ–Ω–∞–º:
        1) –°–Ω–∞—á–∞–ª–∞ –±–µ—Ä—ë–º –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –¥–æ–º–µ–Ω–æ–≤, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ recent_sources.
        2) –ó–∞—Ç–µ–º ‚Äî –∑–∞–ø–æ–ª–Ω—è–µ–º –æ—Å—Ç–∞—Ç–æ–∫, –∏–∑–±–µ–≥–∞—è –ø–æ–¥—Ä—è–¥ –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö –¥–æ–º–µ–Ω–æ–≤.
        """
        if not news_items:
            return []

        # –£–±–µ—Ä—ë–º —è–≤–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ (url,title)
        uniq = {}
        for n in news_items:
            key = (canon_url(n["url"]), normalize_title(n["title"]).lower())
            if key not in uniq:
                uniq[key] = n
        items = list(uniq.values())

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –ø–æ—è–≤–ª–µ–Ω–∏—è –≤ —Å–±–æ—Ä–∫–µ (–∫–∞–∫ –ø—Ä–∏—à–ª–∏) ‚Äî –º–æ–∂–Ω–æ –ø–µ—Ä–µ–º–µ—à–∞—Ç—å
        random.shuffle(items)

        recent = set(self.recent_sources)
        first_pass = [n for n in items if n["domain"] not in recent]
        second_pass = [n for n in items if n["domain"] in recent]

        result: List[Dict] = []
        used_domains: Set[str] = set()

        def take_from(bucket: List[Dict]):
            nonlocal result, used_domains
            for n in bucket:
                if len(result) >= k:
                    break
                d = n["domain"]
                if result and result[-1]["domain"] == d:
                    continue  # –Ω–µ —Å—Ç–∞–≤–∏–º –ø–æ–¥—Ä—è–¥
                result.append(n)
                used_domains.add(d)

        take_from(first_pass)
        if len(result) < k:
            take_from(second_pass)

        # –µ—Å–ª–∏ –≤—Å—ë –µ—â—ë –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç ‚Äî –¥–æ–±–∏—Ä–∞–µ–º —á–µ–º –µ—Å—Ç—å, –∞–∫–∫—É—Ä–∞—Ç–Ω–æ —á–µ—Ä–µ–¥—É—è
        if len(result) < k:
            leftovers = [n for n in items if n not in result]
            for n in leftovers:
                if len(result) >= k:
                    break
                if not result or result[-1]["domain"] != n["domain"]:
                    result.append(n)

        # –æ–±–Ω–æ–≤–∏–º –Ω–µ–¥–∞–≤–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        for n in result:
            self.recent_sources.append(n["domain"])
        self.save_recent_sources()

        return result[:k]

    # ---------- publish ----------

    async def publish_post(self, title: str, content: str, url: str, source: str = "") -> bool:
        if self.is_duplicate(url, title):
            logger.info(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ (–¥—É–±–ª–∏–∫–∞—Ç): {title[:60]}...")
            return False
        if not self.is_finance_related(title, content):
            logger.info(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ (–Ω–µ —Ñ–∏–Ω—Ç–µ–º–∞—Ç–∏–∫–∞): {title[:60]}...")
            return False

        full_text = await self.fetch_full_article_text(url)
        use_text = full_text if full_text.strip() else content
        cleaned = self.clean_text(use_text)
        if len(cleaned) < MIN_CONTENT_LENGTH:
            logger.info(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ (–º–∞–ª–æ —Ç–µ–∫—Å—Ç–∞): {title[:60]}...")
            return False

        max_retries = 3
        for attempt in range(max_retries):
            try:
                message = self.format_message(title, use_text, url)
                await self.bot.send_message(
                    chat_id=self.channel_id,
                    text=message,
                    parse_mode='HTML',
                    disable_web_page_preview=True  # –≤–∞–∂–Ω—ã–π —Ñ–ª–∞–≥ –ø—Ä–æ—Ç–∏–≤ ¬´–º—É—Å–æ—Ä–∞¬ª –∏–∑ –ø—Ä–µ–≤—å—é
                )
                logger.info(f"‚úÖ –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ: {title[:60]}...")
                self.save_hash(url, title)
                if source:
                    self.source_priority[source] = self.source_priority.get(source, 0) + 1
                    self.save_source_stats()
                return True
            except error.RetryAfter as e:
                logger.warning(f"Rate limit, –∂–¥—ë–º {e.retry_after} —Å–µ–∫...")
                await asyncio.sleep(e.retry_after)
            except error.TimedOut:
                logger.warning(f"–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏, –ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries}")
                await asyncio.sleep(2 ** attempt)
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ '{title}': {e}")
                if attempt == max_retries - 1:
                    if source:
                        self.deleted_posts_tracker[source] = datetime.now()
                        self.source_priority[source] = self.source_priority.get(source, 0) - 2
                        self.save_source_stats()
                    return False
                await asyncio.sleep(2 ** attempt)
        return False

    # ---------- scheduling ----------

    def generate_post_schedule(self) -> List[datetime]:
        try:
            msk = pytz.timezone('Europe/Moscow')
            now = datetime.now(msk)
            start_hour, end_hour = 9, 21
            if now.hour < start_hour:
                base = now.replace(hour=start_hour, minute=0, second=0, microsecond=0)
            elif now.hour >= end_hour:
                base = (now + timedelta(days=1)).replace(hour=start_hour, minute=0, second=0, microsecond=0)
            else:
                base = now

            times = []
            available = (end_hour - start_hour) * 60
            slots = MAX_POSTS_PER_DAY
            for i in range(slots):
                minute_position = available * (i + 1) / (slots + 1)
                random_variation = random.uniform(-25, 25)
                total_minutes = minute_position + random_variation
                h = int(total_minutes // 60)
                m = int(total_minutes % 60)
                t = base.replace(hour=start_hour + h, minute=m, second=0, microsecond=0)
                if t.hour < start_hour:
                    t = t.replace(hour=start_hour, minute=random.randint(0, 30))
                elif t.hour >= end_hour:
                    t = t.replace(hour=end_hour - 1, minute=random.randint(30, 59))
                times.append(t)
            return sorted(times)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è: {e}")
            return [datetime.now() + timedelta(minutes=30 * i) for i in range(MAX_POSTS_PER_DAY)]

    # ---------- main ----------

    async def run(self):
        connector = aiohttp.TCPConnector(limit=10, ttl_dns_cache=300)
        timeout = aiohttp.ClientTimeout(total=20)

        async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
            self.session = session

            # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ —Å–æ–±–∏—Ä–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏
            async def _fetch(url):
                if url in self.failed_sources:
                    return []
                try:
                    return await self.fetch_feed(url)
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ {url}: {e}")
                    self.failed_sources.add(url)
                    return []

            tasks = [_fetch(src) for src in RSS_SOURCES]
            results = await asyncio.gather(*tasks, return_exceptions=False)
            all_news = [item for sub in results for item in sub]

            if len(all_news) < MAX_POSTS_PER_DAY:
                logger.info("–ü—Ä–æ–±—É–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏‚Ä¶")
                tasks_b = [_fetch(src) for src in BACKUP_SOURCES]
                results_b = await asyncio.gather(*tasks_b, return_exceptions=False)
                all_news.extend(item for sub in results_b for item in sub)

            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Ç–µ–º–∞—Ç–∏–∫–µ –∏ –¥–ª–∏–Ω–µ
            filtered = []
            seen_urls = set()
            for it in all_news:
                url_c = canon_url(it["url"])
                if url_c in seen_urls:
                    continue
                if (not self.is_duplicate(it["url"], it["title"])
                    and self.is_finance_related(it["title"], it["content"])
                    and len(self.clean_text(it["content"])) >= MIN_CONTENT_LENGTH):
                    filtered.append(it)
                    seen_urls.add(url_c)

            if not filtered:
                logger.info("–ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π.")
                return

            # –°–ø—Ä–∞–≤–µ–¥–ª–∏–≤–∞—è —Ä–æ—Ç–∞—Ü–∏—è –ø–æ –¥–æ–º–µ–Ω–∞–º/–∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
            final_news = self.select_news_fair(filtered, MAX_POSTS_PER_DAY)

            if not final_news:
                logger.info("–ù–µ—á–µ–≥–æ –ø—É–±–ª–∏–∫–æ–≤–∞—Ç—å –ø–æ—Å–ª–µ —Ä–æ—Ç–∞—Ü–∏–∏.")
                return

            schedule = self.generate_post_schedule()
            logger.info(f"–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ –Ω–∞ {len(schedule)} –ø—É–±–ª–∏–∫–∞—Ü–∏–π.")
            for i, t in enumerate(schedule, 1):
                logger.info(f"  {i}. {t.strftime('%Y-%m-%d %H:%M:%S')} –ú–°–ö")

            for i, (news_item, pub_time) in enumerate(zip(final_news, schedule)):
                now = datetime.now(pytz.timezone('Europe/Moscow'))
                if pub_time > now:
                    wait_seconds = (pub_time - now).total_seconds()
                    logger.info(f"–û–∂–∏–¥–∞–Ω–∏–µ –¥–æ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ {i+1}: {int(wait_seconds)} —Å–µ–∫")
                    await asyncio.sleep(wait_seconds)

                ok = await self.publish_post(
                    title=news_item["title"],
                    content=news_item["content"],
                    url=news_item["url"],
                    source=news_item["source"]
                )
                # –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –¥–æ–º–µ–Ω –≤ –∏—Å—Ç–æ—Ä–∏—é —Ä–æ—Ç–∞—Ü–∏–∏ –¥–∞–∂–µ –ø—Ä–∏ –Ω–µ—É–¥–∞—á–µ ‚Äî —á—Ç–æ–±—ã –ø–æ–ø—ã—Ç–∞—Ç—å—Å—è –¥—Ä—É–≥–æ–π
                self.recent_sources.append(news_item["domain"])
                self.save_recent_sources()

                if not ok and i < len(final_news) - 1:
                    logger.warning("–ü—É–±–ª–∏–∫–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å, –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–π.")

                if i < len(final_news) - 1:
                    await asyncio.sleep(random.uniform(5, 12))

            logger.info("‚úÖ –¶–∏–∫–ª –ø—É–±–ª–∏–∫–∞—Ü–∏–π –∑–∞–≤–µ—Ä—à—ë–Ω.")

            if len(self.failed_sources) > 0 and datetime.now().hour == 0:
                logger.info("–û—á–∏—Å—Ç–∫–∞ —Å–ø–∏—Å–∫–∞ –Ω–µ—É–¥–∞—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
                self.failed_sources.clear()


async def main():
    try:
        bot = NewsBot(BOT_TOKEN, CHANNEL_ID)
        await bot.run()
    except KeyboardInterrupt:
        logger.info("–ë–æ—Ç –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        raise


if __name__ == "__main__":
    asyncio.run(main())
