import asyncio
import hashlib
import html
import json
import logging
import os
import random
import re
from datetime import datetime, timedelta
from typing import List, Dict, Set, Optional, Tuple
from urllib.parse import urlparse

import aiohttp
import feedparser
import pytz
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from telegram import Bot, error

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    handlers=[
        logging.FileHandler('bot.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# –û—Ç–∫–ª—é—á–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ URL –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ Telegram API (—á—Ç–æ–±—ã –Ω–µ —Å–≤–µ—Ç–∏—Ç—å —Ç–æ–∫–µ–Ω)
logging.getLogger("httpx").setLevel(logging.WARNING)

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
RSS_SOURCES = [
    "https://www.finam.ru/analytics/rsspoint/",
    "https://www.cbr.ru/rss/eventrss",
    "https://www.vedomosti.ru/rss/rubric/finance/banks.xml",
    "https://arb.ru/rss/news/",
    "https://www.kommersant.ru/RSS/news.xml",
    "https://www.bfm.ru/news.rss?rubric=28",
    "https://ria.ru/export/rss2/archive/index.xml"
]

# –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–∞ —Å–ª—É—á–∞–π –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ –æ—Å–Ω–æ–≤–Ω—ã—Ö
BACKUP_SOURCES = [
    "https://www.kommersant.ru/RSS/bank.xml",
    "https://www.rbc.ru/rss/economics.xml",
    "https://www.interfax.ru/rss.asp",
    "https://www.vestifinance.ru/rss/news"
]

BOT_TOKEN = os.getenv("BOT_TOKEN")
CHANNEL_ID = os.getenv("CHANNEL_ID")

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
if not BOT_TOKEN:
    raise ValueError("‚ùå BOT_TOKEN –Ω–µ –∑–∞–¥–∞–Ω –≤ .env —Ñ–∞–π–ª–µ!")
if not CHANNEL_ID:
    raise ValueError("‚ùå CHANNEL_ID –Ω–µ –∑–∞–¥–∞–Ω –≤ .env —Ñ–∞–π–ª–µ!")

# –í–∞–ª–∏–¥–∞—Ü–∏—è CHANNEL_ID: –¥–ª—è —Å—É–ø–µ—Ä–≥—Ä—É–ø–ø –∏ –∫–∞–Ω–∞–ª–æ–≤ –¥–æ–ª–∂–µ–Ω –Ω–∞—á–∏–Ω–∞—Ç—å—Å—è —Å -100
if CHANNEL_ID.lstrip('-').isdigit() and len(CHANNEL_ID.lstrip('-')) >= 10 and not CHANNEL_ID.startswith('-100'):
    CHANNEL_ID = '-100' + CHANNEL_ID.lstrip('-')

MAX_POSTS_PER_DAY = 3
MAX_CONTENT_LENGTH = 800
MIN_CONTENT_LENGTH = 50

# User-Agent –¥–ª—è –æ–±—Ö–æ–¥–∞ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36'
]

HEADERS = {
    'User-Agent': random.choice(USER_AGENTS)
}

# –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
FINANCE_KEYWORDS = [
    '–±–∞–Ω–∫', '–∫—Ä–µ–¥–∏—Ç', '–∏–ø–æ—Ç–µ–∫–∞', '–≤–∫–ª–∞–¥', '–¥–µ–ø–æ–∑–∏—Ç', '–∞–∫—Ü–∏—è', '–æ–±–ª–∏–≥–∞—Ü–∏—è',
    '—Ä—É–±–ª—å', '–¥–æ–ª–ª–∞—Ä', '–µ–≤—Ä–æ', '–∏–Ω—Ñ–ª—è—Ü–∏—è', '—Å—Ç–∞–≤–∫–∞', '–¶–ë', '–§–†–°', '–±–∏—Ä–∂–∞',
    '–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞', '–Ω–µ—Ñ—Ç—å', '–≥–∞–∑', '—ç–∫–æ–Ω–æ–º–∏–∫–∞', '—Ä—ã–Ω–æ–∫', '–∏–Ω–≤–µ—Å—Ç', '—Ñ–∏–Ω–∞–Ω—Å',
    '–ª–∏–∫–≤–∏–¥–Ω–æ—Å—Ç—å', '–¥–∏–≤–∏–¥–µ–Ω–¥', '–∫—Ä–∏–∑–∏—Å', '—Å–∞–Ω–∫—Ü–∏–∏', '—Ä–µ–≥—É–ª—è—Ç–æ—Ä', '—Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π –±–∞–Ω–∫',
    '–∫—Ä–µ–¥–∏—Ç–Ω–∞—è', '–∑–∞–µ–º', '–∑–∞–π–º', '—Ä–µ—Ñ–∏–Ω–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–∏–µ', '–∏–ø–æ—Ç–µ—á–Ω—ã–π', '–≤–∫–ª–∞–¥–Ω–æ–π',
    '—Å–±–µ—Ä–µ–∂–µ–Ω–∏—è', '–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã–π', '—Ñ–æ–Ω–¥–æ–≤—ã–π', '–≤–∞–ª—é—Ç–Ω—ã–π', '–∫—É—Ä—Å', '–æ–±–º–µ–Ω',
    '–ø–ª–∞—Ç–µ–∂', '–ø–µ—Ä–µ–≤–æ–¥', '–∫–∞—Ä—Ç–∞', '–¥–µ–±–µ—Ç–æ–≤–∞—è', '–∫—Ä–µ–¥–∏—Ç–Ω–∞—è –∫–∞—Ä—Ç–∞', '–±—Ä–æ–∫–µ—Ä',
    '—Ç—Ä–µ–π–¥–∏–Ω–≥', '–∫–æ—Ç–∏—Ä–æ–≤–∫–∏', '–∏–Ω–¥–µ–∫—Å', '—Ä—ã–Ω–æ—á–Ω–∞—è', '–∫–∞–ø–∏—Ç–∞–ª–∏–∑–∞—Ü–∏—è', '–∞–∫—Ç–∏–≤',
    '–ø–∞—Å—Å–∏–≤', '–±–∞–ª–∞–Ω—Å', '–æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å', '–ø—Ä–∏–±—ã–ª—å', '—É–±—ã—Ç–æ–∫', '–¥–∏–≤–∏–¥–µ–Ω–¥—ã',
    '–≤—ã–ø–ª–∞—Ç—ã', '–∫–≤–∞—Ä—Ç–∞–ª—å–Ω—ã–π', '–≥–æ–¥–æ–≤–æ–π', '–æ—Ç—á–µ—Ç', '–∞—É–¥–∏—Ç', '–Ω–∞–¥–∑–æ—Ä', '–ª–∏—Ü–µ–Ω–∑–∏—è',
    '–æ—Ç–∑—ã–≤ –ª–∏—Ü–µ–Ω–∑–∏–∏', '—Å–∞–Ω–∞—Ü–∏—è', '–±–∞–Ω–∫—Ä–æ—Ç—Å—Ç–≤–æ', '—Ñ–æ—Ä–µ–∫—Å', '—Ç—Ä–µ–π–¥–µ—Ä', '–∏–Ω–≤–µ—Å—Ç–æ—Ä',
    '–ø–æ—Ä—Ç—Ñ–µ–ª—å', '–¥–∏–≤–µ—Ä—Å–∏—Ñ–∏–∫–∞—Ü–∏—è', '—Ä–∏—Å–∫', '–¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å', '–ø—Ä–æ—Ü–µ–Ω—Ç', '—Å—Ç–∞–≤–∫–∞ —Ä–µ—Ñ–∏–Ω–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–∏—è',
    '–∫–ª—é—á–µ–≤–∞—è —Å—Ç–∞–≤–∫–∞', '–º–æ–Ω–µ—Ç–∞—Ä–Ω—ã–π', '—Ñ–∏—Å–∫–∞–ª—å–Ω—ã–π', '–±—é–¥–∂–µ—Ç', '–Ω–∞–ª–æ–≥', '—Å–±–æ—Ä',
    '—Ç–∞—Ä–∏—Ñ', '—Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ', '—Å—Ç—Ä–∞—Ö–æ–≤–∞—è', '–ø–µ–Ω—Å–∏–æ–Ω–Ω—ã–π', '–Ω–∞–∫–æ–ø–∏—Ç–µ–ª—å–Ω—ã–π', '–∏–ø–æ—Ç–µ—á–Ω–æ–µ –∫—Ä–µ–¥–∏—Ç–æ–≤–∞–Ω–∏–µ',
    '–ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å—Å–∫–æ–µ –∫—Ä–µ–¥–∏—Ç–æ–≤–∞–Ω–∏–µ', '–º–∏–∫—Ä–æ–∫—Ä–µ–¥–∏—Ç', '–ú–§–û', '–ª–∏–∑–∏–Ω–≥', '—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥'
]

# –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ ‚Üí —Ö–µ—à—Ç–µ–≥–∏ –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π —Ç–µ–º–∞—Ç–∏–∫–∏
KEYWORDS_TO_HASHTAGS = {
    "–±–∞–Ω–∫": ["#–±–∞–Ω–∫–∏", "#—Ñ–∏–Ω–∞–Ω—Å—ã", "#–±–∞–Ω–∫–æ–≤—Å–∫–∏–π–°–µ–∫—Ç–æ—Ä"],
    "–∫—Ä–µ–¥–∏—Ç": ["#–∫—Ä–µ–¥–∏—Ç", "#–∫—Ä–µ–¥–∏—Ç–æ–≤–∞–Ω–∏–µ", "#–∑–∞–π–º—ã"],
    "–∏–ø–æ—Ç–µ–∫–∞": ["#–∏–ø–æ—Ç–µ–∫–∞", "#–Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å", "#–∂–∏–ª—å–µ"],
    "–≤–∫–ª–∞–¥": ["#–≤–∫–ª–∞–¥—ã", "#–¥–µ–ø–æ–∑–∏—Ç—ã", "#—Å–±–µ—Ä–µ–∂–µ–Ω–∏—è"],
    "–∞–∫—Ü–∏—è": ["#–∞–∫—Ü–∏–∏", "#–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏", "#—Ñ–æ–Ω–¥–æ–≤—ã–π–†—ã–Ω–æ–∫"],
    "–æ–±–ª–∏–≥–∞—Ü–∏—è": ["#–æ–±–ª–∏–≥–∞—Ü–∏–∏", "#–±–æ–Ω–¥—Å", "#–¥–æ–ª–≥–æ–≤—ã–µ–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã"],
    "—Ä—É–±–ª—å": ["#—Ä—É–±–ª—å", "#–≤–∞–ª—é—Ç–∞", "#–∫—É—Ä—Å–†—É–±–ª—è"],
    "–¥–æ–ª–ª–∞—Ä": ["#–¥–æ–ª–ª–∞—Ä", "#USD", "#–≤–∞–ª—é—Ç–∞"],
    "–µ–≤—Ä–æ": ["#–µ–≤—Ä–æ", "#EUR", "#–≤–∞–ª—é—Ç–∞"],
    "–∏–Ω—Ñ–ª—è—Ü–∏—è": ["#–∏–Ω—Ñ–ª—è—Ü–∏—è", "#—Ü–µ–Ω—ã", "#—ç–∫–æ–Ω–æ–º–∏–∫–∞"],
    "–∫–ª—é—á–µ–≤–∞—è —Å—Ç–∞–≤–∫–∞": ["#–∫–ª—é—á–µ–≤–∞—è–°—Ç–∞–≤–∫–∞", "#–¶–ë", "#–ø—Ä–æ—Ü–µ–Ω—Ç–Ω–∞—è–°—Ç–∞–≤–∫–∞"],
    "–¶–ë": ["#–¶–µ–Ω—Ç—Ä–æ–±–∞–Ω–∫", "#—Ä–µ–≥—É–ª—è—Ç–æ—Ä", "#–±–∞–Ω–∫–†–æ—Å—Å–∏–∏"],
    "–§–†–°": ["#–§–†–°", "#–§–µ–¥–µ—Ä–∞–ª—å–Ω–∞—è–†–µ–∑–µ—Ä–≤–Ω–∞—è–°–∏—Å—Ç–µ–º–∞", "#–°–®–ê"],
    "–±–∏—Ä–∂–∞": ["#–±–∏—Ä–∂–∞", "#—Ç—Ä–µ–π–¥–∏–Ω–≥", "#—Ñ–æ–Ω–¥–æ–≤—ã–π–†—ã–Ω–æ–∫"],
    "–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞": ["#–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞", "#–±–∏—Ç–∫–æ–∏–Ω", "#–±–ª–æ–∫—á–µ–π–Ω"],
    "–Ω–µ—Ñ—Ç—å": ["#–Ω–µ—Ñ—Ç—å", "#–Ω–µ—Ñ—Ç—è–Ω—ã–µ–ö–æ—Ç–∏—Ä–æ–≤–∫–∏", "#—ç–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞"],
    "–≥–∞–∑": ["#–≥–∞–∑", "#—ç–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞", "#–ì–∞–∑–ø—Ä–æ–º"],
    "—ç–∫–æ–Ω–æ–º–∏–∫–∞": ["#—ç–∫–æ–Ω–æ–º–∏–∫–∞", "#–º–∞–∫—Ä–æ—ç–∫–æ–Ω–æ–º–∏–∫–∞", "#–í–í–ü"],
    "—Ä—ã–Ω–æ–∫": ["#—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π–†—ã–Ω–æ–∫", "#—Ä—ã–Ω–æ–∫", "#—Ç—Ä–µ–π–¥–µ—Ä—ã"],
    "–∏–Ω–≤–µ—Å—Ç": ["#–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏", "#–∏–Ω–≤–µ—Å—Ç–æ—Ä—ã", "#–∫–∞–ø–∏—Ç–∞–ª–æ–≤–ª–æ–∂–µ–Ω–∏—è"],
    "–ª–∏–∫–≤–∏–¥–Ω–æ—Å—Ç—å": ["#–ª–∏–∫–≤–∏–¥–Ω–æ—Å—Ç—å", "#–¥–µ–Ω–µ–∂–Ω—ã–π–†—ã–Ω–æ–∫", "#—Ñ–∏–Ω–∞–Ω—Å—ã"],
    "–¥–∏–≤–∏–¥–µ–Ω–¥": ["#–¥–∏–≤–∏–¥–µ–Ω–¥—ã", "#–¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å", "#–∞–∫—Ü–∏–æ–Ω–µ—Ä—ã"],
    "–∫—Ä–∏–∑–∏—Å": ["#–∫—Ä–∏–∑–∏—Å", "#—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–π–ö—Ä–∏–∑–∏—Å", "#—Ä–µ—Ü–µ—Å—Å–∏—è"],
    "—Å–∞–Ω–∫—Ü–∏–∏": ["#—Å–∞–Ω–∫—Ü–∏–∏", "#—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–µ–°–∞–Ω–∫—Ü–∏–∏", "#–º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ–û—Ç–Ω–æ—à–µ–Ω–∏—è"],
    "—Ä–µ–≥—É–ª—è—Ç–æ—Ä": ["#—Ä–µ–≥—É–ª—è—Ç–æ—Ä", "#–Ω–∞–¥–∑–æ—Ä", "#—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π–ù–∞–¥–∑–æ—Ä"],
}

# –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ —ç–º–æ–¥–∑–∏ –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π —Ç–µ–º–∞—Ç–∏–∫–∏
TOPIC_TO_EMOJI = {
    "–±–∞–Ω–∫": "üè¶",
    "–∫—Ä–µ–¥–∏—Ç": "üí≥",
    "–∏–ø–æ—Ç–µ–∫–∞": "üè†",
    "–≤–∫–ª–∞–¥": "üí∞",
    "–∞–∫—Ü–∏—è": "üìà",
    "–æ–±–ª–∏–≥–∞—Ü–∏—è": "üìä",
    "—Ä—É–±–ª—å": "‚ÇΩ",
    "–¥–æ–ª–ª–∞—Ä": "üíµ",
    "–µ–≤—Ä–æ": "üí∂",
    "–∏–Ω—Ñ–ª—è—Ü–∏—è": "üìâ",
    "–∫–ª—é—á–µ–≤–∞—è —Å—Ç–∞–≤–∫–∞": "üìå",
    "–¶–ë": "üá∑üá∫",
    "–§–†–°": "üá∫üá∏",
    "–±–∏—Ä–∂–∞": "üìä",
    "–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞": "‚Çø",
    "–Ω–µ—Ñ—Ç—å": "üõ¢Ô∏è",
    "–≥–∞–∑": "üî•",
    "—ç–∫–æ–Ω–æ–º–∏–∫–∞": "üåê",
    "—Ä—ã–Ω–æ–∫": "ü§ù",
    "–∏–Ω–≤–µ—Å—Ç": "üíº",
    "–ª–∏–∫–≤–∏–¥–Ω–æ—Å—Ç—å": "üíß",
    "–¥–∏–≤–∏–¥–µ–Ω–¥": "üéÅ",
    "–∫—Ä–∏–∑–∏—Å": "‚ö†Ô∏è",
    "—Å–∞–Ω–∫—Ü–∏–∏": "üö´",
    "—Ä–µ–≥—É–ª—è—Ç–æ—Ä": "üëÆ",
}


class NewsBot:
    def __init__(self, bot_token: str, channel_id: str):
        self.bot = Bot(token=bot_token)
        self.channel_id = channel_id
        self.session: Optional[aiohttp.ClientSession] = None
        self.posted_hashes: Set[str] = set()
        self.failed_sources: Set[str] = set()
        self.source_priority: Dict[str, int] = {}
        self.deleted_posts_tracker: Dict[str, datetime] = {}
        self.last_publication_time: Optional[datetime] = None
        
        self.load_hashes()
        self.load_source_stats()

    def load_hashes(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ö–µ—à–∏ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π"""
        try:
            if os.path.exists('posted_hashes.txt'):
                with open('posted_hashes.txt', 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                    # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 500 —Å—Ç—Ä–æ–∫ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è —Ä–∞–∑—Ä–∞—Å—Ç–∞–Ω–∏—è
                    recent_lines = lines[-500:] if len(lines) > 500 else lines
                    self.posted_hashes = set(line.strip() for line in recent_lines if line.strip())
                logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.posted_hashes)} —Ö–µ—à–µ–π –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏.")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ö–µ—à–µ–π: {e}")

    def load_source_stats(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º"""
        try:
            if os.path.exists('source_stats.json'):
                with open('source_stats.json', 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.source_priority = data.get('priority', {})
                    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Å—Ç—Ä–æ–∫–∏ –¥–∞—Ç –æ–±—Ä–∞—Ç–Ω–æ –≤ datetime
                    deleted_data = data.get('deleted', {})
                    self.deleted_posts_tracker = {
                        source: datetime.fromisoformat(date_str)
                        for source, date_str in deleted_data.items()
                    }
                logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è {len(self.source_priority)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {e}")

    def save_source_stats(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º"""
        try:
            data = {
                'priority': self.source_priority,
                'deleted': {
                    source: date.isoformat()
                    for source, date in self.deleted_posts_tracker.items()
                }
            }
            with open('source_stats.json', 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {e}")

    def save_hash(self, url: str, title: str):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ö–µ—à –Ω–æ–≤–æ—Å—Ç–∏"""
        url_hash = hashlib.md5(url.encode()).hexdigest()
        title_hash = hashlib.md5(title.encode()).hexdigest()
        combined = f"{url_hash}_{title_hash}"
        self.posted_hashes.add(combined)
        try:
            with open('posted_hashes.txt', 'a', encoding='utf-8') as f:
                f.write(combined + '\n')
        except Exception as e:
            logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ö–µ—à: {e}")

    def is_duplicate(self, url: str, title: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ —Ö–µ—à—É URL –∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞"""
        url_hash = hashlib.md5(url.encode()).hexdigest()
        title_hash = hashlib.md5(title.encode()).hexdigest()
        combined = f"{url_hash}_{title_hash}"
        return combined in self.posted_hashes

    def track_deleted_post(self, source: str):
        """–û—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç —É–¥–∞–ª–µ–Ω–Ω—ã–µ –ø–æ—Å—Ç—ã –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        now = datetime.now()
        self.deleted_posts_tracker[source] = now
        
        # –ü–æ–Ω–∏–∂–∞–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–∞, –µ—Å–ª–∏ –µ–≥–æ –ø–æ—Å—Ç—ã —É–¥–∞–ª—è—é—Ç—Å—è
        self.source_priority[source] = self.source_priority.get(source, 0) - 2
        self.save_source_stats()
        logger.info(f"–ü–æ–Ω–∏–∂–µ–Ω –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–∞ {urlparse(source).netloc}")

    def get_source_weight(self, source: str) -> float:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤–µ—Å –∏—Å—Ç–æ—á–Ω–∏–∫–∞ —Å —É—á–µ—Ç–æ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ –∏ –≤—Ä–µ–º–µ–Ω–∏ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —É–¥–∞–ª–µ–Ω–∏—è"""
        base_weight = 1.0
        
        # –£—á–µ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
        priority = self.source_priority.get(source, 0)
        priority_factor = max(0.1, 1.0 + (priority * 0.1))
        
        # –£—á–µ—Ç –≤—Ä–µ–º–µ–Ω–∏ —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —É–¥–∞–ª–µ–Ω–∏—è
        if source in self.deleted_posts_tracker:
            last_deleted = self.deleted_posts_tracker[source]
            hours_since_deletion = (datetime.now() - last_deleted).total_seconds() / 3600
            # –ß–µ–º –±–æ–ª—å—à–µ –≤—Ä–µ–º–µ–Ω–∏ –ø—Ä–æ—à–ª–æ, —Ç–µ–º –≤—ã—à–µ –≤–µ—Å
            time_factor = min(2.0, 1.0 + (hours_since_deletion / 24))
        else:
            time_factor = 1.0
        
        return base_weight * priority_factor * time_factor

    def prioritize_sources(self, news_items: List[Dict]) -> List[Dict]:
        """–ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä—É–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ —Å —É—á–µ—Ç–æ–º –≤–µ—Å–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        if not news_items:
            return []
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
        news_by_source = {}
        for item in news_items:
            source = item["source"]
            if source not in news_by_source:
                news_by_source[source] = []
            news_by_source[source].append(item)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Å–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        source_weights = {}
        for source in news_by_source.keys():
            source_weights[source] = self.get_source_weight(source)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –ø–æ –≤–µ—Å—É (–≤ –ø–æ—Ä—è–¥–∫–µ —É–±—ã–≤–∞–Ω–∏—è)
        sorted_sources = sorted(news_by_source.keys(), 
                               key=lambda x: source_weights[x], 
                               reverse=True)
        
        # –û—Ç–±–∏—Ä–∞–µ–º –ª—É—á—à–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –∫–∞–∂–¥–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        selected_news = []
        for source in sorted_sources:
            # –ë–µ—Ä–µ–º –Ω–µ –±–æ–ª–µ–µ 1 –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –∑–∞ —Ü–∏–∫–ª
            if news_by_source[source]:
                selected_news.append(news_by_source[source][0])
        
        return selected_news[:MAX_POSTS_PER_DAY]

    def is_finance_related(self, title: str, content: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –ª–∏ –Ω–æ–≤–æ—Å—Ç—å –∫ –±–∞–Ω–∫–æ–≤—Å–∫–æ–π/—Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π —Ç–µ–º–∞—Ç–∏–∫–µ"""
        text = f"{title} {content}".lower()
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ–≥–æ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞
        return any(keyword in text for keyword in FINANCE_KEYWORDS)

    @staticmethod
    def clean_text(text: str) -> str:
        """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç HTML, –∞–≤—Ç–æ—Ä–æ–≤, —Ä–µ–∫–ª–∞–º—ã, –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –º—É—Å–æ—Ä–∞"""
        if not text:
            return ""

        soup = BeautifulSoup(text, "html.parser")

        # –£–¥–∞–ª—è–µ–º —Ä–µ–∫–ª–∞–º–Ω—ã–µ –∏ –Ω–∞–≤–∏–≥–∞—Ü–∏–æ–Ω–Ω—ã–µ –±–ª–æ–∫–∏
        for elem in soup.find_all(['script', 'style', 'nav', 'header', 'footer', 'aside', 'advertisement', 'iframe', 'form']):
            elem.decompose()

        # –£–¥–∞–ª—è–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–ª–∞–º–Ω—ã–µ –≤—Å—Ç–∞–≤–∫–∏
        for clip in soup.find_all(class_=lambda x: x and any(word in str(x).lower() for word in ['clip', 'ad', 'banner', 'promo', 'recommended', 'social', 'share'])):
            clip.decompose()

        # –£–¥–∞–ª—è–µ–º —Å–ø–∏—Å–∫–∏ –±–∞–Ω–∫–æ–≤, –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –∏ –ø—Ä–æ—á–µ–≥–æ –º—É—Å–æ—Ä–∞
        for ul in soup.find_all('ul'):
            text_content = ul.get_text()
            if any(keyword in text_content.lower() for keyword in ['–±–∞–Ω–∫', '–≤–∫–ª–∞–¥', '–∫—Ä–µ–¥–∏—Ç', '–∫–∞—Ä—Ç–∞', '–∏–ø–æ—Ç–µ–∫–∞', '—Ä–µ–∫–ª–∞–º']):
                ul.decompose()

        text = soup.get_text()
        text = html.unescape(text)

        # –£–¥–∞–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∞–≤—Ç–æ—Ä–µ, –∏—Å—Ç–æ—á–Ω–∏–∫–µ, –¥–∞—Ç–µ –∏ –ø—Ä–æ—á–∏–π –º—É—Å–æ—Ä
        patterns = [
            r'^\s*[–ê-–Ø][–∞-—è]+\s+[–ê-–Ø]\.[–ê-–Ø]\.?',
            r'^\s*[–ê-–Ø][–∞-—è]+(?:\s+[–ê-–Ø][–∞-—è]+)?\s*/\s*[–ê-–Ø][–∞-—è]+',
            r'^\s*‚Äî\s*[^\n]+',
            r'(–§–æ—Ç–æ|–ò–ª–ª—é—Å—Ç—Ä–∞—Ü–∏—è|–ò—Å—Ç–æ—á–Ω–∏–∫|–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π|–ê–≤—Ç–æ—Ä|–ß–∏—Ç–∞—Ç—å –ø–æ–¥—Ä–æ–±–Ω–µ–µ|–†–µ–¥–∞–∫—Ü–∏—è|–ö–æ—Ä—Ä–µ—Å–ø–æ–Ω–¥–µ–Ω—Ç)[:\s].*?(?=\s*[–ê-–Ø])',
            r'^\s*[–ê-–Ø]\.\s*',
            r'^\s*–¶–µ–Ω—ã –Ω–∞.*?(?=\s*[–ê-–Ø])',
            r'^\s*–ü–æ.*?–Ω–∞ –ø—Ä–æ—à–ª–æ–π –Ω–µ–¥–µ–ª–µ.*?(?=\s*[–ê-–Ø])',
            r'^\s*\d{1,2}\s+[–ê-–Ø–∞-—è]+\s+\d{4}[^–ê-–Ø–∞-—è]*',
            r'–ï—â—ë –≤–∏–¥–µ–æ.*?(?=\s*[–ê-–Ø])',
            r'^\s*[–ê-–Ø–∞-—è]+\s+(?:–Ω–æ–≤–æ—Å—Ç–∏|–ø—Ä–µ—Å—Å-—Ä–µ–ª–∏–∑|—Å—Ç—Ä–∞—Ç–µ–≥–∏–∏)\b',
            r'\* –†–µ–π—Ç–∏–Ω–≥ —Å–æ—Å—Ç–∞–≤–ª–µ–Ω.*',
            r'–†–µ–π—Ç–∏–Ω–≥ —Å–æ—Å—Ç–∞–≤–ª–µ–Ω.*',
            r'–ß—Ç–æ–±—ã –Ω–µ.*',
            r'–ò–Ω–Ω–∞ –°–æ–ª–¥–∞—Ç–µ–Ω–∫–æ–≤–∞.*',
            r'–≠–∫—Å–ø–µ—Ä—Ç-–∞–Ω–∞–ª–∏—Ç–∏–∫.*',
            r'–ê–Ω–∞–ª–∏—Ç–∏–∫ –ë–∞–Ω–∫–∏\.—Ä—É.*',
            r'–†–∞—Å—Å—á–∏—Ç–∞—Ç—å —Å—É–º–º—É.*',
            r'https?://\S+',
            r'\d{1,2}:\d{2}',  # –≤—Ä–µ–º—è
            r'\d{1,2}\s*[–∞-—è]+\s+\d{4}',  # –¥–∞—Ç—ã
            r'–ß–∏—Ç–∞–π—Ç–µ —Ç–∞–∫–∂–µ.*',
            r'–†–µ–∫–ª–∞–º–∞.*',
            r'–ú–∞—Ç–µ—Ä–∏–∞–ª.*–ø–∞—Ä—Ç–Ω–µ—Ä–æ–≤',
            r'–û–±—Å—É–¥–∏—Ç—å –≤ —Ç–µ–ª–µ–≥—Ä–∞–º–µ.*',
            r'–ü–æ–¥–ø–∏—à–∏—Ç–µ—Å—å –Ω–∞.*',
            r'–ú—ã –≤ —Å–æ—Ü—Å–µ—Ç—è—Ö.*',
            r'–ü—Ä–∏—Å–ª–∞—Ç—å –Ω–æ–≤–æ—Å—Ç—å.*',
            r'–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏.*',
            r'–¢–µ–ª–µ–≥—Ä–∞–º-–∫–∞–Ω–∞–ª.*',
            r'–ü–æ–¥–ø–∏—Å—ã–≤–∞–π—Ç–µ—Å—å.*',
        ]

        for pattern in patterns:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE | re.DOTALL)

        # –£–¥–∞–ª—è–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–∑ 3+ –≤–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –∏–ª–∏ –≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤
        text = re.sub(r'[!?]{3,}', ' ', text)

        # –ó–∞–º–µ–Ω—è–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫ –Ω–∞ –æ–¥–∏–Ω –ø—Ä–æ–±–µ–ª
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    def extract_hashtags(self, title: str, content: str, creator: str = "") -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ö–µ—à—Ç–µ–≥–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ (–º–∞–∫—Å. 5)"""
        text = f"{title} {content} {creator}".lower()

        hashtags = set()
        for keyword, tags in KEYWORDS_TO_HASHTAGS.items():
            if keyword.lower() in text:
                hashtags.update(tags)

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π —Ç–µ–º–∞—Ç–∏–∫–∏
        extra = []
        if any(w in text for w in ["–±–∏—Ä–∂–∞", "—Ç—Ä–µ–π–¥–∏–Ω–≥", "–∏–Ω–≤–µ—Å—Ç"]):
            extra.append("#–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏")
        if any(w in text for w in ["–∫—Ä–∏–ø—Ç–æ", "–±–∏—Ç–∫–æ–∏–Ω", "–±–ª–æ–∫—á–µ–π–Ω"]):
            extra.append("#–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç—ã")
        if any(w in text for w in ["–Ω–µ—Ñ—Ç—å", "–≥–∞–∑", "—ç–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞"]):
            extra.append("#—ç–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞")
        if any(w in text for w in ["—Å–∞–Ω–∫—Ü–∏–∏", "—ç–º–±–∞—Ä–≥–æ", "–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è"]):
            extra.append("#–º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ–û—Ç–Ω–æ—à–µ–Ω–∏—è")

        hashtags.update(extra)
        return sorted(set(hashtags))[:5]

    def get_relevant_emoji(self, title: str, content: str, creator: str = "") -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π —ç–º–æ–¥–∑–∏"""
        text = f"{title} {content} {creator}".lower()
        for keyword, emoji in sorted(TOPIC_TO_EMOJI.items(), key=lambda x: len(x[0]), reverse=True):
            if keyword.lower() in text:
                return emoji
        return "üì∞"

    async def fetch_full_article_text(self, url: str) -> str:
        """–ü–∞—Ä—Å–∏—Ç –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ —Å –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        max_retries = 3
        for attempt in range(max_retries):
            try:
                headers = HEADERS.copy()
                headers['User-Agent'] = random.choice(USER_AGENTS)

                async with self.session.get(url, headers=headers, timeout=15) as response:
                    if response.status != 200:
                        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç–∞—Ç—å—é {url}: —Å—Ç–∞—Ç—É—Å {response.status}")
                        if attempt < max_retries - 1:
                            await asyncio.sleep(2 ** attempt)
                        continue

                    html_text = await response.text()
                    soup = BeautifulSoup(html_text, "html.parser")

                    for elem in soup.find_all(['script', 'style', 'nav', 'header', 'footer', 'aside', 'advertisement', 'iframe', 'form']):
                        elem.decompose()

                    # –£–¥–∞–ª—è–µ–º —Ä–µ–∫–ª–∞–º–Ω—ã–µ –±–ª–æ–∫–∏
                    for ad in soup.find_all(class_=lambda x: x and any(word in str(x).lower() for word in ['ad', 'banner', 'promo', 'recommended', 'social', 'share'])):
                        ad.decompose()

                    domain = urlparse(url).netloc
                    selectors = []

                    if "finam.ru" in domain:
                        selectors = ['.article__body', '.content', 'article']
                    elif "cbr.ru" in domain:
                        selectors = ['.content', '.text', 'article']
                    elif "vedomosti.ru" in domain:
                        selectors = ['.article__body', '.article-body', 'article']
                    elif "arb.ru" in domain:
                        selectors = ['.news-detail', '.content', 'article']
                    elif "kommersant.ru" in domain:
                        selectors = ['.article__text', '.article-text', 'article']
                    elif "rbc.ru" in domain:
                        selectors = ['.article__text', '.article__content', 'article']
                    elif "banki.ru" in domain:
                        selectors = ['.news-text', '.article-content', 'article']
                    else:
                        selectors = ['.article-content', '.post-content', '.entry-content',
                                   '.article__body', '.article-body', 'article', '.content']

                    content = None
                    for selector in selectors:
                        try:
                            if selector.startswith('.'):
                                content = soup.find(class_=selector[1:])
                            elif selector.startswith('#'):
                                content = soup.find(id=selector[1:])
                            else:
                                content = soup.find(selector)
                            if content:
                                break
                        except Exception:
                            continue

                    if not content:
                        all_texts = soup.find_all(text=True)
                        text_blocks = [t.parent for t in all_texts if len(t.strip()) > 50]
                        text_blocks.sort(key=lambda x: len(x.get_text()), reverse=True)
                        content = text_blocks[0] if text_blocks else soup.find('body')

                    if content:
                        return self.clean_text(content.get_text())
                    return ""

            except asyncio.TimeoutError:
                logger.warning(f"–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —Å—Ç–∞—Ç—å–∏ {url}, –ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(2 ** attempt)
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —Å—Ç–∞—Ç—å–∏ {url}: {e}, –ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(2 ** attempt)

        return ""

    def smart_truncate(self, text: str, max_length: int = MAX_CONTENT_LENGTH) -> str:
        """–û–±—Ä–µ–∑–∞–µ—Ç —Ç–µ–∫—Å—Ç –ø–æ –≥—Ä–∞–Ω–∏—Ü–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π"""
        if len(text) <= max_length:
            return text
        truncated = text[:max_length + 1]
        last_end = max(truncated.rfind('.'), truncated.rfind('!'), truncated.rfind('?'))
        if last_end > max_length - 100:
            return truncated[:last_end + 1]
        else:
            return truncated[:max_length] + "‚Ä¶"

    def format_message(self, title: str, content: str, url: str, creator: str = "") -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ —Å HTML-—Ä–∞–∑–º–µ—Ç–∫–æ–π, –ø—Ä–æ–≤–µ—Ä—è—è –¥–ª–∏–Ω—É –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É—è –∞–±–∑–∞—Ü—ã"""
        # –£–¥–∞–ª—è–µ–º –¥–∞—Ç—ã –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞
        title = re.sub(r'\s*\d{1,2}\s+[–ê-–Ø–∞-—è]+\s+\d{4}\s*–≥–æ–¥–∞?\b', '', title, flags=re.IGNORECASE).strip()

        full_text = self.clean_text(content)
        if full_text.startswith(title):
            full_text = full_text[len(title):].lstrip(":.- ")

        truncated_content = self.smart_truncate(full_text, MAX_CONTENT_LENGTH)

        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏ —Ñ–æ—Ä–º–∏—Ä—É–µ–º –∞–±–∑–∞—Ü—ã –ø–æ 1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = re.split(r'(?<=[.!?])\s+', truncated_content)
        paragraphs = []
        current_paragraph = ""

        for sentence in sentences:
            if not current_paragraph:
                current_paragraph = sentence
            elif len(current_paragraph.split('. ')) < 2:  # –ï—Å–ª–∏ –≤ –∞–±–∑–∞—Ü–µ –º–µ–Ω—å—à–µ 2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
                current_paragraph += " " + sentence
            else:
                paragraphs.append(current_paragraph.strip())
                current_paragraph = sentence

        if current_paragraph:
            paragraphs.append(current_paragraph.strip())

        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∞–±–∑–∞—Ü—ã
        formatted_content = "\n\n".join(f"{p}" for p in paragraphs if len(p) > 20)

        hashtags = self.extract_hashtags(title, content, creator)
        hashtag_line = "\n\n" + " ".join(hashtags) if hashtags else ""
        emoji = self.get_relevant_emoji(title, content, creator)

        message = (
            f"<b>{emoji} {title}</b>\n\n"
            f"{formatted_content}\n\n"
            f"üëâ <a href='{url}'>–ß–∏—Ç–∞—Ç—å –¥–∞–ª–µ–µ</a>"
            f"{hashtag_line}"
        )

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã —Å–æ–æ–±—â–µ–Ω–∏—è (–ª–∏–º–∏—Ç Telegram ~4096 —Å–∏–º–≤–æ–ª–æ–≤)
        if len(message) > 3900:
            message = message[:3897] + "..."

        return message

    async def fetch_feed(self, url: str) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ RSS-–ª–µ–Ω—Ç—ã —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ—á–∏—Å—Ç–∫–æ–π –∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        max_retries = 3
        for attempt in range(max_retries):
            try:
                headers = HEADERS.copy()
                headers['User-Agent'] = random.choice(USER_AGENTS)

                async with self.session.get(url, headers=headers, timeout=15) as response:
                    if response.status != 200:
                        logger.warning(f"–û—à–∏–±–∫–∞ HTTP {response.status} –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {url}")
                        self.failed_sources.add(url)
                        if attempt < max_retries - 1:
                            await asyncio.sleep(2 ** attempt)
                        continue

                    content = await response.text()
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º to_thread –¥–ª—è —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ feedparser
                    feed = await asyncio.to_thread(feedparser.parse, content)
                    entries = []

                    for entry in feed.entries:
                        title = entry.get("title", "").strip()
                        if "–≤–∏–¥–µ–æ" in title.lower() or "video" in title.lower():
                            continue

                        # –û—á–∏—Å—Ç–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞
                        title = re.sub(r'^(.*?)(?:\s*-\s*[–ê-–Ø–∞-—è]+)*$', r'\1', title).strip()
                        link = entry.get("link", "").strip()
                        description = entry.get("description", "") or entry.get("summary", "")

                        if "<![CDATA[" in description:
                            description = re.sub(r'<!\[CDATA\[(.*?)\]\]>', r'\1', description, flags=re.DOTALL)

                        # –û—á–∏—Å—Ç–∫–∞ –æ–ø–∏—Å–∞–Ω–∏—è
                        soup_desc = BeautifulSoup(description, "html.parser")
                        description = soup_desc.get_text()

                        description = re.sub(r'^\s*[–ê-–Ø]\.\s*[–ê-–Ø][–∞-—è]+\s*/\s*[–ê-–Ø][–∞-—è]+', '', description)
                        description = re.sub(r'^\s*[–ê-–Ø][–∞-—è]+\s+[–ê-–Ø]\.\s*', '', description)
                        description = re.sub(r'^\s*(?:–ê–≤—Ç–æ—Ä|–ò—Å—Ç–æ—á–Ω–∏–∫):?\s*[^\s]+\s*', '', description, flags=re.IGNORECASE)

                        creator = ""
                        if hasattr(entry, "tags") and entry.tags:
                            creator = " ".join([tag.term for tag in entry.tags[:2]])
                        elif hasattr(entry, "author"):
                            creator = entry.author
                        elif hasattr(entry, "dc") and hasattr(entry.dc, "creator"):
                            creator = entry.dc.creator
                        elif hasattr(entry, "creator"):
                            creator = entry.creator

                        if title and link and self.is_finance_related(title, description):
                            entries.append({
                                "title": title,
                                "url": link,
                                "content": description,
                                "creator": creator or "",
                                "source": url
                            })

                    logger.info(f"–ü–æ–ª—É—á–µ–Ω–æ {len(entries)} –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ {urlparse(url).netloc}")
                    return entries

            except asyncio.TimeoutError:
                logger.warning(f"–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —Ñ–∏–¥–∞ {url}, –ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(2 ** attempt)
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {url}: {e}, –ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}")
                self.failed_sources.add(url)
                if attempt < max_retries - 1:
                    await asyncio.sleep(2 ** attempt)

        return []

    async def publish_post(self, title: str, content: str, url: str, creator: str = "", source: str = "") -> bool:
        """–ü—É–±–ª–∏–∫—É–µ—Ç –ø–æ—Å—Ç —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        if self.is_duplicate(url, title):
            logger.info(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ (–¥—É–±–ª–∏–∫–∞—Ç): {title[:50]}...")
            return False

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—É—é —Ç–µ–º–∞—Ç–∏–∫—É
        if not self.is_finance_related(title, content):
            logger.info(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ (–Ω–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–∞—è —Ç–µ–º–∞—Ç–∏–∫–∞): {title[:50]}...")
            return False

        full_text = await self.fetch_full_article_text(url)
        use_text = full_text if full_text.strip() else content

        cleaned_text = self.clean_text(use_text)
        if len(cleaned_text) < MIN_CONTENT_LENGTH:
            logger.info(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ (–º–∞–ª–æ —Ç–µ–∫—Å—Ç–∞): {title[:50]}...")
            return False

        max_retries = 3
        for attempt in range(max_retries):
            try:
                message = self.format_message(title, use_text, url, creator)
                await self.bot.send_message(
                    chat_id=self.channel_id,
                    text=message,
                    parse_mode='HTML',
                    disable_web_page_preview=False
                )
                logger.info(f"‚úÖ –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ: {title[:50]}...")
                self.save_hash(url, title)
                
                # –ü–æ–≤—ã—à–∞–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç —É—Å–ø–µ—à–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
                if source:
                    self.source_priority[source] = self.source_priority.get(source, 0) + 1
                    self.save_source_stats()
                
                return True
            except error.RetryAfter as e:
                logger.warning(f"‚ö†Ô∏è –ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç, –æ–∂–∏–¥–∞–Ω–∏–µ {e.retry_after} —Å–µ–∫...")
                await asyncio.sleep(e.retry_after)
            except error.TimedOut:
                logger.warning(f"‚ö†Ô∏è –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏, –ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries}")
                await asyncio.sleep(2 ** attempt)
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ '{title}' (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}): {e}")
                if attempt == max_retries - 1:
                    # –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º –Ω–µ—É–¥–∞—á–Ω—ã–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
                    if source:
                        self.track_deleted_post(source)
                    return False
                await asyncio.sleep(2 ** attempt)

        return False

    def generate_post_schedule(self) -> List[datetime]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–π —Å—Ç—Ä–æ–≥–æ —Å 9:00 –¥–æ 21:00 –ø–æ –ú—Å–∫"""
        try:
            moscow_tz = pytz.timezone('Europe/Moscow')
            now_moscow = datetime.now(moscow_tz)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–µ –æ–∫–Ω–æ (9:00-21:00 –ø–æ –ú—Å–∫)
            start_hour, end_hour = 9, 21
            
            # –ï—Å–ª–∏ —Å–µ–π—á–∞—Å –≤–Ω–µ —Ä–∞–±–æ—á–µ–≥–æ –≤—Ä–µ–º–µ–Ω–∏, –Ω–∞—á–∏–Ω–∞–µ–º —Å 9:00 —Å–ª–µ–¥—É—é—â–µ–≥–æ –¥–Ω—è
            if now_moscow.hour < start_hour:
                first_post_time = now_moscow.replace(
                    hour=start_hour, minute=0, second=0, microsecond=0
                )
            elif now_moscow.hour >= end_hour:
                first_post_time = now_moscow.replace(
                    hour=start_hour, minute=0, second=0, microsecond=0
                ) + timedelta(days=1)
            else:
                # –¢–µ–∫—É—â–µ–µ –≤—Ä–µ–º—è –≤ —Ä–∞–±–æ—á–µ–º –æ–∫–Ω–µ
                first_post_time = now_moscow
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤—Ä–µ–º–µ–Ω–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–π
            times = []
            available_minutes = (end_hour - start_hour) * 60
            time_slots = MAX_POSTS_PER_DAY
            
            # –†–∞–≤–Ω–æ–º–µ—Ä–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –≤ —Ç–µ—á–µ–Ω–∏–µ –¥–Ω—è
            for i in range(time_slots):
                # –í—ã—á–∏—Å–ª—è–µ–º –ø–æ–∑–∏—Ü–∏—é –≤ –º–∏–Ω—É—Ç–Ω–æ–º –∏–Ω—Ç–µ—Ä–≤–∞–ª–µ
                minute_position = available_minutes * (i + 1) / (time_slots + 1)
                random_variation = random.uniform(-30, 30)  # ¬±30 –º–∏–Ω—É—Ç –≤–∞—Ä–∏–∞—Ü–∏—è
                
                total_minutes = minute_position + random_variation
                hours_to_add = int(total_minutes // 60)
                minutes_to_add = int(total_minutes % 60)
                
                post_time = first_post_time.replace(
                    hour=start_hour + hours_to_add,
                    minute=minutes_to_add,
                    second=0,
                    microsecond=0
                )
                
                # –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ–º, —á—Ç–æ–±—ã –≤—Ä–µ–º—è –±—ã–ª–æ –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö 9:00-21:00
                if post_time.hour < start_hour:
                    post_time = post_time.replace(hour=start_hour, minute=random.randint(0, 30))
                elif post_time.hour >= end_hour:
                    post_time = post_time.replace(hour=end_hour - 1, minute=random.randint(30, 59))
                
                times.append(post_time)
            
            return sorted(times)
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è: {e}")
            # –†–µ–∑–µ—Ä–≤–Ω–æ–µ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ –Ω–∞ —Å–ª—É—á–∞–π –æ—à–∏–±–∫–∏
            return [datetime.now() + timedelta(minutes=30 * i) for i in range(MAX_POSTS_PER_DAY)]

    def avoid_consecutive_sources(self, posts: List[Dict]) -> List[Dict]:
        """–ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ—Ç –ø–æ—Å—Ç—ã, —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ –ø–æ–¥—Ä—è–¥ –∏–∑ –æ–¥–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        if len(posts) < 2:
            return posts
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ—Å—Ç—ã –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
        posts_by_source = {}
        for post in posts:
            source = post["source"]
            if source not in posts_by_source:
                posts_by_source[source] = []
            posts_by_source[source].append(post)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –ø–æ—Å—Ç–æ–≤
        sorted_sources = sorted(posts_by_source.keys(), key=lambda x: len(posts_by_source[x]), reverse=True)
        
        result = []
        while any(posts_by_source.values()):
            for source in sorted_sources:
                if posts_by_source[source]:
                    # –ë–µ—Ä–µ–º –ø–æ—Å—Ç –∏–∑ —Ç–µ–∫—É—â–µ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
                    post = posts_by_source[source].pop(0)
                    result.append(post)
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –∏–¥—É—Ç –ª–∏ –¥–≤–∞ –ø–æ—Å—Ç–∞ –ø–æ–¥—Ä—è–¥ –∏–∑ –æ–¥–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
                    if len(result) >= 2 and result[-2]["source"] == result[-1]["source"]:
                        # –ï—Å–ª–∏ –¥–∞, –∏—â–µ–º –ø–æ—Å—Ç –∏–∑ –¥—Ä—É–≥–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏
                        for other_source in sorted_sources:
                            if other_source != source and posts_by_source[other_source]:
                                insert_post = posts_by_source[other_source].pop(0)
                                result.insert(-1, insert_post)
                                break
        
        return result[:MAX_POSTS_PER_DAY]

    async def run(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –±–æ—Ç–∞ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π —Ä–æ—Ç–∞—Ü–∏–µ–π –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        connector = aiohttp.TCPConnector(limit=5, ttl_dns_cache=300)
        timeout = aiohttp.ClientTimeout(total=20)

        async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
            self.session = session

            all_news = []
            seen_urls = set()

            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
            for source in RSS_SOURCES:
                try:
                    if source in self.failed_sources:
                        continue

                    news = await self.fetch_feed(source)
                    for item in news:
                        if item["url"] not in seen_urls:
                            all_news.append(item)
                            seen_urls.add(item["url"])
                    await asyncio.sleep(2)  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ {source}: {e}")
                    self.failed_sources.add(source)

            # –ï—Å–ª–∏ –Ω–æ–≤–æ—Å—Ç–µ–π –º–∞–ª–æ, –ø—Ä–æ–±—É–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
            if len(all_news) < MAX_POSTS_PER_DAY:
                logger.info("–ü—Ä–æ–±—É–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏...")
                for backup_source in BACKUP_SOURCES:
                    try:
                        if backup_source in self.failed_sources:
                            continue

                        news = await self.fetch_feed(backup_source)
                        for item in news:
                            if item["url"] not in seen_urls:
                                all_news.append(item)
                                seen_urls.add(item["url"])
                        await asyncio.sleep(2)
                    except Exception as e:
                        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ä–µ–∑–µ—Ä–≤–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞{backup_source}: {e}")
                        self.failed_sources.add(backup_source)

            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π
            filtered_news = []
            for item in all_news:
                if (not self.is_duplicate(item["url"], item["title"]) and
                    self.is_finance_related(item["title"], item["content"]) and
                    len(self.clean_text(item["content"])) >= MIN_CONTENT_LENGTH):
                    filtered_news.append(item)

            # –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Å —É—á–µ—Ç–æ–º –∏—Å—Ç–æ—Ä–∏–∏ —É–¥–∞–ª–µ–Ω–∏–π
            prioritized_news = self.prioritize_sources(filtered_news)

            if not prioritized_news:
                logger.info("–ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏.")
                return

            # –ò–∑–±–µ–≥–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –ø—É–±–ª–∏–∫–∞—Ü–∏–π –∏–∑ –æ–¥–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
            final_news = self.avoid_consecutive_sources(prioritized_news)

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–π
            schedule = self.generate_post_schedule()
            
            logger.info(f"–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ –Ω–∞ {len(schedule)} –ø—É–±–ª–∏–∫–∞—Ü–∏–π:")
            for i, pub_time in enumerate(schedule, 1):
                logger.info(f"  {i}. {pub_time.strftime('%Y-%m-%d %H:%M:%S')}")

            # –ü—É–±–ª–∏–∫—É–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—é
            for i, (news_item, pub_time) in enumerate(zip(final_news, schedule)):
                # –ñ–¥–µ–º –¥–æ –≤—Ä–µ–º–µ–Ω–∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
                now = datetime.now(pytz.timezone('Europe/Moscow'))
                if pub_time > now:
                    wait_seconds = (pub_time - now).total_seconds()
                    logger.info(f"–û–∂–∏–¥–∞–Ω–∏–µ –¥–æ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ {i+1}: {int(wait_seconds)} —Å–µ–∫—É–Ω–¥")
                    await asyncio.sleep(wait_seconds)

                # –ü—É–±–ª–∏–∫—É–µ–º –Ω–æ–≤–æ—Å—Ç—å
                success = await self.publish_post(
                    title=news_item["title"],
                    content=news_item["content"],
                    url=news_item["url"],
                    creator=news_item.get("creator", ""),
                    source=news_item["source"]
                )

                if not success and i < len(final_news) - 1:
                    logger.warning("–ü—É–±–ª–∏–∫–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å, –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–π –Ω–æ–≤–æ—Å—Ç–∏")
                    continue

                # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –ø—É–±–ª–∏–∫–∞—Ü–∏—è–º–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å –µ—â–µ –Ω–æ–≤–æ—Å—Ç–∏)
                if i < len(final_news) - 1:
                    await asyncio.sleep(random.uniform(5, 15))

            logger.info("‚úÖ –¶–∏–∫–ª –ø—É–±–ª–∏–∫–∞—Ü–∏–π –∑–∞–≤–µ—Ä—à–µ–Ω")

            # –û—á–∏—Å—Ç–∫–∞ —Å–ø–∏—Å–∫–∞ –Ω–µ—É–¥–∞—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Ä–∞–∑ –≤ –¥–µ–Ω—å
            if len(self.failed_sources) > 0 and datetime.now().hour == 0:
                logger.info("–û—á–∏—Å—Ç–∫–∞ —Å–ø–∏—Å–∫–∞ –Ω–µ—É–¥–∞—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
                self.failed_sources.clear()


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞ –±–æ—Ç–∞"""
    try:
        bot = NewsBot(BOT_TOKEN, CHANNEL_ID)
        await bot.run()
    except KeyboardInterrupt:
        logger.info("–ë–æ—Ç –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        raise


if __name__ == "__main__":
    asyncio.run(main())