import asyncio
import hashlib
import html
import logging
import os
import random
import re
from datetime import datetime, timedelta
from typing import List, Dict, Set, Optional
from urllib.parse import urlparse

import aiohttp
import feedparser
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from telegram import Bot, error

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    handlers=[
        logging.FileHandler('bot.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# –û—Ç–∫–ª—é—á–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ URL –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ Telegram API (—á—Ç–æ–±—ã –Ω–µ —Å–≤–µ—Ç–∏—Ç—å —Ç–æ–∫–µ–Ω)
logging.getLogger("httpx").setLevel(logging.WARNING)

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
RSS_SOURCES = [
    "https://www.finam.ru/analytics/rsspoint/",
    "https://www.cbr.ru/rss/eventrss",
    "https://www.vedomosti.ru/rss/rubric/finance/banks.xml",
    "https://arb.ru/rss/news/",
    "https://www.kommersant.ru/RSS/news.xml",
    "https://www.rbc.ru/rss/finances.xml",
    "https://www.banki.ru/xml/news.rss"
]

# –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–∞ —Å–ª—É—á–∞–π –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ –æ—Å–Ω–æ–≤–Ω—ã—Ö
BACKUP_SOURCES = [
    "https://www.kommersant.ru/RSS/bank.xml",
    "https://www.rbc.ru/rss/economics.xml",
    "https://www.interfax.ru/rss.asp",
    "https://www.vestifinance.ru/rss/news"
]

BOT_TOKEN = os.getenv("BOT_TOKEN")
CHANNEL_ID = os.getenv("CHANNEL_ID")

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
if not BOT_TOKEN:
    raise ValueError("‚ùå BOT_TOKEN –Ω–µ –∑–∞–¥–∞–Ω –≤ .env —Ñ–∞–π–ª–µ!")
if not CHANNEL_ID:
    raise ValueError("‚ùå CHANNEL_ID –Ω–µ –∑–∞–¥–∞–Ω –≤ .env —Ñ–∞–π–ª–µ!")

# –í–∞–ª–∏–¥–∞—Ü–∏—è CHANNEL_ID: –¥–ª—è —Å—É–ø–µ—Ä–≥—Ä—É–ø–ø –∏ –∫–∞–Ω–∞–ª–æ–≤ –¥–æ–ª–∂–µ–Ω –Ω–∞—á–∏–Ω–∞—Ç—å—Å—è —Å -100
if CHANNEL_ID.lstrip('-').isdigit() and len(CHANNEL_ID.lstrip('-')) >= 10 and not CHANNEL_ID.startswith('-100'):
    CHANNEL_ID = '-100' + CHANNEL_ID.lstrip('-')

MAX_POSTS_PER_DAY = 4
MAX_CONTENT_LENGTH = 800
MIN_CONTENT_LENGTH = 50

# User-Agent –¥–ª—è –æ–±—Ö–æ–¥–∞ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36'
]

HEADERS = {
    'User-Agent': random.choice(USER_AGENTS)
}

# –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ ‚Üí —Ö–µ—à—Ç–µ–≥–∏ –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π —Ç–µ–º–∞—Ç–∏–∫–∏
KEYWORDS_TO_HASHTAGS = {
    "–±–∞–Ω–∫": ["#–±–∞–Ω–∫–∏", "#—Ñ–∏–Ω–∞–Ω—Å—ã", "#–±–∞–Ω–∫–æ–≤—Å–∫–∏–π–°–µ–∫—Ç–æ—Ä"],
    "–∫—Ä–µ–¥–∏—Ç": ["#–∫—Ä–µ–¥–∏—Ç", "#–∫—Ä–µ–¥–∏—Ç–æ–≤–∞–Ω–∏–µ", "#–∑–∞–π–º—ã"],
    "–∏–ø–æ—Ç–µ–∫–∞": ["#–∏–ø–æ—Ç–µ–∫–∞", "#–Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å", "#–∂–∏–ª—å–µ"],
    "–≤–∫–ª–∞–¥": ["#–≤–∫–ª–∞–¥—ã", "#–¥–µ–ø–æ–∑–∏—Ç—ã", "#—Å–±–µ—Ä–µ–∂–µ–Ω–∏—è"],
    "–∞–∫—Ü–∏—è": ["#–∞–∫—Ü–∏–∏", "#–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏", "#—Ñ–æ–Ω–¥–æ–≤—ã–π–†—ã–Ω–æ–∫"],
    "–æ–±–ª–∏–≥–∞—Ü–∏—è": ["#–æ–±–ª–∏–≥–∞—Ü–∏–∏", "#–±–æ–Ω–¥—Å", "#–¥–æ–ª–≥–æ–≤—ã–µ–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã"],
    "—Ä—É–±–ª—å": ["#—Ä—É–±–ª—å", "#–≤–∞–ª—é—Ç–∞", "#–∫—É—Ä—Å–†—É–±–ª—è"],
    "–¥–æ–ª–ª–∞—Ä": ["#–¥–æ–ª–ª–∞—Ä", "#USD", "#–≤–∞–ª—é—Ç–∞"],
    "–µ–≤—Ä–æ": ["#–µ–≤—Ä–æ", "#EUR", "#–≤–∞–ª—é—Ç–∞"],
    "–∏–Ω—Ñ–ª—è—Ü–∏—è": ["#–∏–Ω—Ñ–ª—è—Ü–∏—è", "#—Ü–µ–Ω—ã", "#—ç–∫–æ–Ω–æ–º–∏–∫–∞"],
    "–∫–ª—é—á–µ–≤–∞—è —Å—Ç–∞–≤–∫–∞": ["#–∫–ª—é—á–µ–≤–∞—è–°—Ç–∞–≤–∫–∞", "#–¶–ë", "#–ø—Ä–æ—Ü–µ–Ω—Ç–Ω–∞—è–°—Ç–∞–≤–∫–∞"],
    "–¶–ë": ["#–¶–µ–Ω—Ç—Ä–æ–±–∞–Ω–∫", "#—Ä–µ–≥—É–ª—è—Ç–æ—Ä", "#–±–∞–Ω–∫–†–æ—Å—Å–∏–∏"],
    "–§–†–°": ["#–§–†–°", "#–§–µ–¥–µ—Ä–∞–ª—å–Ω–∞—è–†–µ–∑–µ—Ä–≤–Ω–∞—è–°–∏—Å—Ç–µ–º–∞", "#–°–®–ê"],
    "–±–∏—Ä–∂–∞": ["#–±–∏—Ä–∂–∞", "#—Ç—Ä–µ–π–¥–∏–Ω–≥", "#—Ñ–æ–Ω–¥–æ–≤—ã–π–†—ã–Ω–æ–∫"],
    "–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞": ["#–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞", "#–±–∏—Ç–∫–æ–∏–Ω", "#–±–ª–æ–∫—á–µ–π–Ω"],
    "–Ω–µ—Ñ—Ç—å": ["#–Ω–µ—Ñ—Ç—å", "#–Ω–µ—Ñ—Ç—è–Ω—ã–µ–ö–æ—Ç–∏—Ä–æ–≤–∫–∏", "#—ç–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞"],
    "–≥–∞–∑": ["#–≥–∞–∑", "#—ç–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞", "#–ì–∞–∑–ø—Ä–æ–º"],
    "—ç–∫–æ–Ω–æ–º–∏–∫–∞": ["#—ç–∫–æ–Ω–æ–º–∏–∫–∞", "#–º–∞–∫—Ä–æ—ç–∫–æ–Ω–æ–º–∏–∫–∞", "#–í–í–ü"],
    "—Ä—ã–Ω–æ–∫": ["#—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π–†—ã–Ω–æ–∫", "#—Ä—ã–Ω–æ–∫", "#—Ç—Ä–µ–π–¥–µ—Ä—ã"],
    "–∏–Ω–≤–µ—Å—Ç": ["#–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏", "#–∏–Ω–≤–µ—Å—Ç–æ—Ä—ã", "#–∫–∞–ø–∏—Ç–∞–ª–æ–≤–ª–æ–∂–µ–Ω–∏—è"],
    "–ª–∏–∫–≤–∏–¥–Ω–æ—Å—Ç—å": ["#–ª–∏–∫–≤–∏–¥–Ω–æ—Å—Ç—å", "#–¥–µ–Ω–µ–∂–Ω—ã–π–†—ã–Ω–æ–∫", "#—Ñ–∏–Ω–∞–Ω—Å—ã"],
    "–¥–∏–≤–∏–¥–µ–Ω–¥": ["#–¥–∏–≤–∏–¥–µ–Ω–¥—ã", "#–¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å", "#–∞–∫—Ü–∏–æ–Ω–µ—Ä—ã"],
    "–∫—Ä–∏–∑–∏—Å": ["#–∫—Ä–∏–∑–∏—Å", "#—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–π–ö—Ä–∏–∑–∏—Å", "#—Ä–µ—Ü–µ—Å—Å–∏—è"],
    "—Å–∞–Ω–∫—Ü–∏–∏": ["#—Å–∞–Ω–∫—Ü–∏–∏", "#—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–µ–°–∞–Ω–∫—Ü–∏–∏", "#–º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ–û—Ç–Ω–æ—à–µ–Ω–∏—è"],
    "—Ä–µ–≥—É–ª—è—Ç–æ—Ä": ["#—Ä–µ–≥—É–ª—è—Ç–æ—Ä", "#–Ω–∞–¥–∑–æ—Ä", "#—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π–ù–∞–¥–∑–æ—Ä"],
}

# –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ —ç–º–æ–¥–∑–∏ –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π —Ç–µ–º–∞—Ç–∏–∫–∏
TOPIC_TO_EMOJI = {
    "–±–∞–Ω–∫": "üè¶",
    "–∫—Ä–µ–¥–∏—Ç": "üí≥",
    "–∏–ø–æ—Ç–µ–∫–∞": "üè†",
    "–≤–∫–ª–∞–¥": "üí∞",
    "–∞–∫—Ü–∏—è": "üìà",
    "–æ–±–ª–∏–≥–∞—Ü–∏—è": "üìä",
    "—Ä—É–±–ª—å": "‚ÇΩ",
    "–¥–æ–ª–ª–∞—Ä": "üíµ",
    "–µ–≤—Ä–æ": "üí∂",
    "–∏–Ω—Ñ–ª—è—Ü–∏—è": "üìâ",
    "–∫–ª—é—á–µ–≤–∞—è —Å—Ç–∞–≤–∫–∞": "üìå",
    "–¶–ë": "üá∑üá∫",
    "–§–†–°": "üá∫üá∏",
    "–±–∏—Ä–∂–∞": "üìä",
    "–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞": "‚Çø",
    "–Ω–µ—Ñ—Ç—å": "üõ¢Ô∏è",
    "–≥–∞–∑": "üî•",
    "—ç–∫–æ–Ω–æ–º–∏–∫–∞": "üåê",
    "—Ä—ã–Ω–æ–∫": "ü§ù",
    "–∏–Ω–≤–µ—Å—Ç": "üíº",
    "–ª–∏–∫–≤–∏–¥–Ω–æ—Å—Ç—å": "üíß",
    "–¥–∏–≤–∏–¥–µ–Ω–¥": "üéÅ",
    "–∫—Ä–∏–∑–∏—Å": "‚ö†Ô∏è",
    "—Å–∞–Ω–∫—Ü–∏–∏": "üö´",
    "—Ä–µ–≥—É–ª—è—Ç–æ—Ä": "üëÆ",
}


class NewsBot:
    def __init__(self, bot_token: str, channel_id: str):
        self.bot = Bot(token=bot_token)
        self.channel_id = channel_id
        self.session: Optional[aiohttp.ClientSession] = None
        self.posted_hashes: Set[str] = set()
        self.failed_sources: Set[str] = set()
        self.load_hashes()

    def load_hashes(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ö–µ—à–∏ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π"""
        try:
            if os.path.exists('posted_hashes.txt'):
                with open('posted_hashes.txt', 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                    # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 500 —Å—Ç—Ä–æ–∫ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è —Ä–∞–∑—Ä–∞—Å—Ç–∞–Ω–∏—è
                    recent_lines = lines[-500:] if len(lines) > 500 else lines
                    self.posted_hashes = set(line.strip() for line in recent_lines if line.strip())
                logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.posted_hashes)} —Ö–µ—à–µ–π –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏.")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ö–µ—à–µ–π: {e}")

    def save_hash(self, url: str, title: str):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ö–µ—à –Ω–æ–≤–æ—Å—Ç–∏"""
        url_hash = hashlib.md5(url.encode()).hexdigest()
        title_hash = hashlib.md5(title.encode()).hexdigest()
        combined = f"{url_hash}_{title_hash}"
        self.posted_hashes.add(combined)
        try:
            with open('posted_hashes.txt', 'a', encoding='utf-8') as f:
                f.write(combined + '\n')
        except Exception as e:
            logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ö–µ—à: {e}")

    def is_duplicate(self, url: str, title: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ —Ö–µ—à—É URL –∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞"""
        url_hash = hashlib.md5(url.encode()).hexdigest()
        title_hash = hashlib.md5(title.encode()).hexdigest()
        combined = f"{url_hash}_{title_hash}"
        return combined in self.posted_hashes

    @staticmethod
    def clean_text(text: str) -> str:
        """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç HTML, –∞–≤—Ç–æ—Ä–æ–≤, —Ä–µ–∫–ª–∞–º—ã, –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –º—É—Å–æ—Ä–∞"""
        if not text:
            return ""

        soup = BeautifulSoup(text, "html.parser")

        # –£–¥–∞–ª—è–µ–º —Ä–µ–∫–ª–∞–º–Ω—ã–µ –∏ –Ω–∞–≤–∏–≥–∞—Ü–∏–æ–Ω–Ω—ã–µ –±–ª–æ–∫–∏ (–æ—Å–æ–±–µ–Ω–Ω–æ –∞–∫—Ç—É–∞–ª—å–Ω–æ –¥–ª—è banki.ru)
        for elem in soup.find_all(['script', 'style', 'nav', 'header', 'footer', 'aside', 'advertisement', 'iframe', 'form']):
            elem.decompose()

        # –£–¥–∞–ª—è–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–ª–∞–º–Ω—ã–µ –≤—Å—Ç–∞–≤–∫–∏ banki.ru
        for clip in soup.find_all(class_=lambda x: x and 'clip' in x):
            clip.decompose()

        # –£–¥–∞–ª—è–µ–º —Å–ø–∏—Å–∫–∏ –±–∞–Ω–∫–æ–≤, –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –∏ –ø—Ä–æ—á–µ–≥–æ –º—É—Å–æ—Ä–∞
        for ul in soup.find_all('ul'):
            text_content = ul.get_text()
            if any(keyword in text_content.lower() for keyword in ['–±–∞–Ω–∫', '–≤–∫–ª–∞–¥', '–∫—Ä–µ–¥–∏—Ç', '–∫–∞—Ä—Ç–∞', '–∏–ø–æ—Ç–µ–∫–∞']):
                ul.decompose()

        text = soup.get_text()
        text = html.unescape(text)

        # –£–¥–∞–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∞–≤—Ç–æ—Ä–µ, –∏—Å—Ç–æ—á–Ω–∏–∫–µ, –¥–∞—Ç–µ –∏ –ø—Ä–æ—á–∏–π –º—É—Å–æ—Ä
        patterns = [
            r'^\s*[–ê-–Ø][–∞-—è]+\s+[–ê-–Ø]\.[–ê-–Ø]\.?',
            r'^\s*[–ê-–Ø][–∞-—è]+(?:\s+[–ê-–Ø][–∞-—è]+)?\s*/\s*[–ê-–Ø][–∞-—è]+',
            r'^\s*‚Äî\s*[^\n]+',
            r'(–§–æ—Ç–æ|–ò–ª–ª—é—Å—Ç—Ä–∞—Ü–∏—è|–ò—Å—Ç–æ—á–Ω–∏–∫|–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π|–ê–≤—Ç–æ—Ä|–ß–∏—Ç–∞—Ç—å –ø–æ–¥—Ä–æ–±–Ω–µ–µ)[:\s].*?(?=\s*[–ê-–Ø])',
            r'^\s*[–ê-–Ø]\.\s*',
            r'^\s*–¶–µ–Ω—ã –Ω–∞.*?(?=\s*[–ê-–Ø])',
            r'^\s*–ü–æ.*?–Ω–∞ –ø—Ä–æ—à–ª–æ–π –Ω–µ–¥–µ–ª–µ.*?(?=\s*[–ê-–Ø])',
            r'^\s*\d{1,2}\s+[–ê-–Ø–∞-—è]+\s+\d{4}[^–ê-–Ø–∞-—è]*',
            r'–ï—â—ë –≤–∏–¥–µ–æ.*?(?=\s*[–ê-–Ø])',
            r'^\s*[–ê-–Ø–∞-—è]+\s+(?:–Ω–æ–≤–æ—Å—Ç–∏|–ø—Ä–µ—Å—Å-—Ä–µ–ª–∏–∑|—Å—Ç—Ä–∞—Ç–µ–≥–∏–∏)\b',
            r'\* –†–µ–π—Ç–∏–Ω–≥ —Å–æ—Å—Ç–∞–≤–ª–µ–Ω.*',
            r'–†–µ–π—Ç–∏–Ω–≥ —Å–æ—Å—Ç–∞–≤–ª–µ–Ω.*',
            r'–ß—Ç–æ–±—ã –Ω–µ.*',
            r'–ò–Ω–Ω–∞ –°–æ–ª–¥–∞—Ç–µ–Ω–∫–æ–≤–∞.*',
            r'–≠–∫—Å–ø–µ—Ä—Ç-–∞–Ω–∞–ª–∏—Ç–∏–∫.*',
            r'–ê–Ω–∞–ª–∏—Ç–∏–∫ –ë–∞–Ω–∫–∏\.—Ä—É.*',
            r'–†–∞—Å—Å—á–∏—Ç–∞—Ç—å —Å—É–º–º—É.*',
            r'https?://\S+',
        ]

        for pattern in patterns:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE | re.DOTALL)

        # –£–¥–∞–ª—è–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–∑ 3+ –≤–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –∏–ª–∏ –≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤
        text = re.sub(r'[!?]{3,}', ' ', text)

        # –ó–∞–º–µ–Ω—è–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫ –Ω–∞ –æ–¥–∏–Ω –ø—Ä–æ–±–µ–ª
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    def extract_hashtags(self, title: str, content: str, creator: str = "") -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ö–µ—à—Ç–µ–≥–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ (–º–∞–∫—Å. 5)"""
        text = f"{title} {content} {creator}".lower()

        hashtags = set()
        for keyword, tags in KEYWORDS_TO_HASHTAGS.items():
            if keyword.lower() in text:
                hashtags.update(tags)

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π —Ç–µ–º–∞—Ç–∏–∫–∏
        extra = []
        if any(w in text for w in ["–±–∏—Ä–∂–∞", "—Ç—Ä–µ–π–¥–∏–Ω–≥", "–∏–Ω–≤–µ—Å—Ç"]):
            extra.append("#–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏")
        if any(w in text for w in ["–∫—Ä–∏–ø—Ç–æ", "–±–∏—Ç–∫–æ–∏–Ω", "–±–ª–æ–∫—á–µ–π–Ω"]):
            extra.append("#–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç—ã")
        if any(w in text for w in ["–Ω–µ—Ñ—Ç—å", "–≥–∞–∑", "—ç–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞"]):
            extra.append("#—ç–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞")
        if any(w in text for w in ["—Å–∞–Ω–∫—Ü–∏–∏", "—ç–º–±–∞—Ä–≥–æ", "–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è"]):
            extra.append("#–º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ–û—Ç–Ω–æ—à–µ–Ω–∏—è")

        hashtags.update(extra)
        return sorted(set(hashtags))[:5]

    def get_relevant_emoji(self, title: str, content: str, creator: str = "") -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π —ç–º–æ–¥–∑–∏"""
        text = f"{title} {content} {creator}".lower()
        for keyword, emoji in sorted(TOPIC_TO_EMOJI.items(), key=lambda x: len(x[0]), reverse=True):
            if keyword.lower() in text:
                return emoji
        return "üì∞"

    async def fetch_full_article_text(self, url: str) -> str:
        """–ü–∞—Ä—Å–∏—Ç –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ —Å –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        max_retries = 3
        for attempt in range(max_retries):
            try:
                headers = HEADERS.copy()
                headers['User-Agent'] = random.choice(USER_AGENTS)

                async with self.session.get(url, headers=headers, timeout=15) as response:
                    if response.status != 200:
                        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç–∞—Ç—å—é {url}: —Å—Ç–∞—Ç—É—Å {response.status}")
                        if attempt < max_retries - 1:
                            await asyncio.sleep(2 ** attempt)
                        continue

                    html_text = await response.text()
                    soup = BeautifulSoup(html_text, "html.parser")

                    for elem in soup.find_all(['script', 'style', 'nav', 'header', 'footer', 'aside', 'advertisement', 'iframe', 'form']):
                        elem.decompose()

                    domain = urlparse(url).netloc
                    selectors = []

                    if "finam.ru" in domain:
                        selectors = ['.article__body', '.content', 'article']
                    elif "cbr.ru" in domain:
                        selectors = ['.content', '.text', 'article']
                    elif "vedomosti.ru" in domain:
                        selectors = ['.article__body', '.article-body', 'article']
                    elif "arb.ru" in domain:
                        selectors = ['.news-detail', '.content', 'article']
                    elif "kommersant.ru" in domain:
                        selectors = ['.article__text', '.article-text', 'article']
                    elif "rbc.ru" in domain:
                        selectors = ['.article__text', '.article__content', 'article']
                    elif "banki.ru" in domain:
                        selectors = ['.news-text', '.article-content', 'article']
                    else:
                        selectors = ['.article-content', '.post-content', '.entry-content',
                                   '.article__body', '.article-body', 'article', '.content']

                    content = None
                    for selector in selectors:
                        try:
                            if selector.startswith('.'):
                                content = soup.find(class_=selector[1:])
                            elif selector.startswith('#'):
                                content = soup.find(id=selector[1:])
                            else:
                                content = soup.find(selector)
                            if content:
                                break
                        except Exception:
                            continue

                    if not content:
                        all_texts = soup.find_all(text=True)
                        text_blocks = [t.parent for t in all_texts if len(t.strip()) > 50]
                        text_blocks.sort(key=lambda x: len(x.get_text()), reverse=True)
                        content = text_blocks[0] if text_blocks else soup.find('body')

                    if content:
                        return self.clean_text(content.get_text())
                    return ""

            except asyncio.TimeoutError:
                logger.warning(f"–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —Å—Ç–∞—Ç—å–∏ {url}, –ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(2 ** attempt)
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —Å—Ç–∞—Ç—å–∏ {url}: {e}, –ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(2 ** attempt)

        return ""

    def smart_truncate(self, text: str, max_length: int = MAX_CONTENT_LENGTH) -> str:
        """–û–±—Ä–µ–∑–∞–µ—Ç —Ç–µ–∫—Å—Ç –ø–æ –≥—Ä–∞–Ω–∏—Ü–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π"""
        if len(text) <= max_length:
            return text
        truncated = text[:max_length + 1]
        last_end = max(truncated.rfind('.'), truncated.rfind('!'), truncated.rfind('?'))
        if last_end > max_length - 100:
            return truncated[:last_end + 1]
        else:
            return truncated[:max_length] + "‚Ä¶"

    def format_message(self, title: str, content: str, url: str, creator: str = "") -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ —Å HTML-—Ä–∞–∑–º–µ—Ç–∫–æ–π, –ø—Ä–æ–≤–µ—Ä—è—è –¥–ª–∏–Ω—É –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É—è –∞–±–∑–∞—Ü—ã"""
        # –£–¥–∞–ª—è–µ–º –¥–∞—Ç—ã –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞
        title = re.sub(r'\s*\d{1,2}\s+[–ê-–Ø–∞-—è]+\s+\d{4}\s*–≥–æ–¥–∞?\b', '', title, flags=re.IGNORECASE).strip()

        full_text = self.clean_text(content)
        if full_text.startswith(title):
            full_text = full_text[len(title):].lstrip(":.- ")

        truncated_content = self.smart_truncate(full_text, MAX_CONTENT_LENGTH)

        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏ —Ñ–æ—Ä–º–∏—Ä—É–µ–º –∞–±–∑–∞—Ü—ã –ø–æ 1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = re.split(r'(?<=[.!?])\s+', truncated_content)
        paragraphs = []
        current_paragraph = ""

        for sentence in sentences:
            if not current_paragraph:
                current_paragraph = sentence
            elif len(current_paragraph.split('. ')) < 2:  # –ï—Å–ª–∏ –≤ –∞–±–∑–∞—Ü–µ –º–µ–Ω—å—à–µ 2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
                current_paragraph += " " + sentence
            else:
                paragraphs.append(current_paragraph.strip())
                current_paragraph = sentence

        if current_paragraph:
            paragraphs.append(current_paragraph.strip())

        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∞–±–∑–∞—Ü—ã
        formatted_content = "\n\n".join(f"{p}" for p in paragraphs if len(p) > 20)

        hashtags = self.extract_hashtags(title, content, creator)
        hashtag_line = "\n\n" + " ".join(hashtags) if hashtags else ""
        emoji = self.get_relevant_emoji(title, content, creator)

        message = (
            f"<b>{emoji} {title}</b>\n\n"
            f"{formatted_content}\n\n"
            f"üëâ <a href='{url}'>–ß–∏—Ç–∞—Ç—å –¥–∞–ª–µ–µ</a>"
            f"{hashtag_line}"
        )

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã —Å–æ–æ–±—â–µ–Ω–∏—è (–ª–∏–º–∏—Ç Telegram ~4096 —Å–∏–º–≤–æ–ª–æ–≤)
        if len(message) > 3900:
            message = message[:3897] + "..."

        return message

    async def fetch_feed(self, url: str) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ RSS-–ª–µ–Ω—Ç—ã —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ—á–∏—Å—Ç–∫–æ–π –∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        max_retries = 3
        for attempt in range(max_retries):
            try:
                headers = HEADERS.copy()
                headers['User-Agent'] = random.choice(USER_AGENTS)

                async with self.session.get(url, headers=headers, timeout=15) as response:
                    if response.status != 200:
                        logger.warning(f"–û—à–∏–±–∫–∞ HTTP {response.status} –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {url}")
                        self.failed_sources.add(url)
                        if attempt < max_retries - 1:
                            await asyncio.sleep(2 ** attempt)
                        continue

                    content = await response.text()
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º to_thread –¥–ª—è —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ feedparser
                    feed = await asyncio.to_thread(feedparser.parse, content)
                    entries = []

                    for entry in feed.entries:
                        title = entry.get("title", "").strip()
                        if "–≤–∏–¥–µ–æ" in title.lower() or "video" in title.lower():
                            continue

                        # –û—á–∏—Å—Ç–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞
                        title = re.sub(r'^(.*?)(?:\s*-\s*[–ê-–Ø–∞-—è]+)*$', r'\1', title).strip()
                        link = entry.get("link", "").strip()
                        description = entry.get("description", "") or entry.get("summary", "")

                        if "<![CDATA[" in description:
                            description = re.sub(r'<!\[CDATA\[(.*?)\]\]>', r'\1', description, flags=re.DOTALL)

                        # –û—á–∏—Å—Ç–∫–∞ –æ–ø–∏—Å–∞–Ω–∏—è
                        soup_desc = BeautifulSoup(description, "html.parser")
                        description = soup_desc.get_text()

                        description = re.sub(r'^\s*[–ê-–Ø]\.\s*[–ê-–Ø][–∞-—è]+\s*/\s*[–ê-–Ø][–∞-—è]+', '', description)
                        description = re.sub(r'^\s*[–ê-–Ø][–∞-—è]+\s+[–ê-–Ø]\.\s*', '', description)
                        description = re.sub(r'^\s*(?:–ê–≤—Ç–æ—Ä|–ò—Å—Ç–æ—á–Ω–∏–∫):?\s*[^\s]+\s*', '', description, flags=re.IGNORECASE)

                        creator = ""
                        if hasattr(entry, "tags") and entry.tags:
                            creator = " ".join([tag.term for tag in entry.tags[:2]])
                        elif hasattr(entry, "author"):
                            creator = entry.author
                        elif hasattr(entry, "dc") and hasattr(entry.dc, "creator"):
                            creator = entry.dc.creator
                        elif hasattr(entry, "creator"):
                            creator = entry.creator

                        if title and link:
                            entries.append({
                                "title": title,
                                "url": link,
                                "content": description,
                                "creator": creator or "",
                                "source": url
                            })

                    logger.info(f"–ü–æ–ª—É—á–µ–Ω–æ {len(entries)} –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ {urlparse(url).netloc}")
                    return entries

            except asyncio.TimeoutError:
                logger.warning(f"–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —Ñ–∏–¥–∞ {url}, –ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(2 ** attempt)
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {url}: {e}, –ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}")
                self.failed_sources.add(url)
                if attempt < max_retries - 1:
                    await asyncio.sleep(2 ** attempt)

        return []

    async def publish_post(self, title: str, content: str, url: str, creator: str = "") -> bool:
        """–ü—É–±–ª–∏–∫—É–µ—Ç –ø–æ—Å—Ç —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        if self.is_duplicate(url, title):
            logger.info(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ (–¥—É–±–ª–∏–∫–∞—Ç): {title[:50]}...")
            return False

        full_text = await self.fetch_full_article_text(url)
        use_text = full_text if full_text.strip() else content

        cleaned_text = self.clean_text(use_text)
        if len(cleaned_text) < MIN_CONTENT_LENGTH:
            logger.info(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ (–º–∞–ª–æ —Ç–µ–∫—Å—Ç–∞): {title[:50]}...")
            return False

        max_retries = 3
        for attempt in range(max_retries):
            try:
                message = self.format_message(title, use_text, url, creator)
                await self.bot.send_message(
                    chat_id=self.channel_id,
                    text=message,
                    parse_mode='HTML',
                    disable_web_page_preview=False
                )
                logger.info(f"‚úÖ –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ: {title[:50]}...")
                self.save_hash(url, title)
                return True
            except error.RetryAfter as e:
                logger.warning(f"‚ö†Ô∏è –ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç, –æ–∂–∏–¥–∞–Ω–∏–µ {e.retry_after} —Å–µ–∫...")
                await asyncio.sleep(e.retry_after)
            except error.TimedOut:
                logger.warning(f"‚ö†Ô∏è –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏, –ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries}")
                await asyncio.sleep(2 ** attempt)
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ '{title}' (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}): {e}")
                if attempt == max_retries - 1:
                    return False
                await asyncio.sleep(2 ** attempt)

        return False

    def generate_post_schedule(self) -> List[datetime]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ: –ø–µ—Ä–≤–∞—è –ø—É–±–ª–∏–∫–∞—Ü–∏—è —Å—Ä–∞–∑—É, –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Å–ª—É—á–∞–π–Ω–æ"""
        now = datetime.now()
        times = [now]  # –ü–µ—Ä–≤–∞—è –ø—É–±–ª–∏–∫–∞—Ü–∏—è —Å—Ä–∞–∑—É

        for _ in range(MAX_POSTS_PER_DAY - 1):
            random_hours = random.randint(1, 6)
            random_minutes = random.randint(0, 59)
            next_time = now + timedelta(hours=random_hours, minutes=random_minutes)

            if next_time.hour >= 22:
                next_time = next_time.replace(hour=21, minute=0)

            times.append(next_time)
            now = next_time

        return sorted(times)

    def avoid_consecutive_sources(self, posts: List[Dict]) -> List[Dict]:
        """–ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ—Ç –ø–æ—Å—Ç—ã, —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ >2 –ø–æ–¥—Ä—è–¥ –∏–∑ –æ–¥–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)"""
        if len(posts) < 3:
            return posts
        # –ü—Ä–æ—Å—Ç–æ –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å–ª—É—á–∞–π–Ω—ã–º –æ–±—Ä–∞–∑–æ–º
        random.shuffle(posts)
        return posts

    async def run(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –±–æ—Ç–∞ —Å –æ–±—Ö–æ–¥–æ–º –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫"""
        connector = aiohttp.TCPConnector(limit=5, ttl_dns_cache=300)
        timeout = aiohttp.ClientTimeout(total=20)

        async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
            self.session = session

            all_news = []
            seen_urls = set()

            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
            for source in RSS_SOURCES:
                try:
                    if source in self.failed_sources:
                        continue

                    news = await self.fetch_feed(source)
                    for item in news:
                        if item["url"] not in seen_urls:
                            all_news.append(item)
                            seen_urls.add(item["url"])
                    await asyncio.sleep(2)  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ {source}: {e}")
                    self.failed_sources.add(source)

            # –ï—Å–ª–∏ –Ω–æ–≤–æ—Å—Ç–µ–π –º–∞–ª–æ, –ø—Ä–æ–±—É–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
            if len(all_news) < MAX_POSTS_PER_DAY:
                logger.info("–ü—Ä–æ–±—É–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏...")
                for backup_source in BACKUP_SOURCES:
                    try:
                        if backup_source in self.failed_sources:
                            continue

                        news = await self.fetch_feed(backup_source)
                        for item in news:
                            if item["url"] not in seen_urls:
                                all_news.append(item)
                                seen_urls.add(item["url"])
                        await asyncio.sleep(2)
                    except Exception as e:
                        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ä–µ–∑–µ—Ä–≤–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ {backup_source}: {e}")
                        self.failed_sources.add(backup_source)

            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
            all_news = [n for n in all_news if not self.is_duplicate(n["url"], n["title"])]

            if not all_news:
                logger.warning("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏")
                return

            if len(all_news) < MAX_POSTS_PER_DAY:
                logger.warning(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: {len(all_news)} –∏–∑ {MAX_POSTS_PER_DAY}")

            random.shuffle(all_news)
            selected_posts = all_news[:MAX_POSTS_PER_DAY]
            final_posts = self.avoid_consecutive_sources(selected_posts)

            schedule = self.generate_post_schedule()
            # –û–±—Ä–µ–∑–∞–µ–º —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ –¥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤
            schedule = schedule[:len(final_posts)]
            logger.info(f"–†–∞—Å–ø–∏—Å–∞–Ω–∏–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–π: {', '.join(t.strftime('%H:%M') for t in schedule)}")

            # –ü—É–±–ª–∏–∫–∞—Ü–∏—è –ø–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—é
            published_count = 0
            for i, post_time in enumerate(schedule):
                now = datetime.now()
                if i > 0:
                    if now < post_time:
                        wait_seconds = (post_time - now).total_seconds()
                        logger.info(f"‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ –¥–æ {post_time.strftime('%H:%M')} ‚Äî {int(wait_seconds)} —Å–µ–∫...")
                        await asyncio.sleep(wait_seconds)

                if i < len(final_posts):
                    post = final_posts[i]
                    success = await self.publish_post(
                        title=post["title"],
                        content=post["content"],
                        url=post["url"],
                        creator=post.get("creator", "")
                    )
                    if success:
                        published_count += 1
                else:
                    logger.warning(f"–ù–µ—Ç –ø–æ—Å—Ç–∞ –¥–ª—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –≤ —Å–ª–æ—Ç {i}")

            logger.info(f"üîö –¶–∏–∫–ª –∑–∞–≤–µ—Ä—à—ë–Ω. –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ {published_count} –∏–∑ {len(final_posts)} –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—É–±–ª–∏–∫–∞—Ü–∏–π.")


# === –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞ ===
async def main():
    logger.info("üöÄ –ë–æ—Ç —É—Å–ø–µ—à–Ω–æ –∑–∞–ø—É—â–µ–Ω –∏ –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ!")

    bot = NewsBot(BOT_TOKEN, CHANNEL_ID)
    try:
        await bot.run()
    except Exception as e:
        logger.critical(f"üí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")


if __name__ == "__main__":
    asyncio.run(main())