import asyncio
import hashlib
import html
import logging
import os
import random
import re
from datetime import datetime, time, timedelta
from typing import List, Dict, Set, Optional
from urllib.parse import urlparse

import aiohttp
import feedparser
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from telegram import Bot, error

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    handlers=[
        logging.FileHandler('bot.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
RSS_SOURCES = [
    "https://www.finam.ru/analytics/rsspoint/",
    "https://www.cbr.ru/rss/eventrss",
    "https://www.vedomosti.ru/rss/rubric/finance/banks.xml",
    "https://arb.ru/rss/news/",
    "https://www.kommersant.ru/RSS/news.xml",
    "https://www.rbc.ru/rss/finances.xml",
    "https://www.banki.ru/xml/news.rss"
]

# –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–∞ —Å–ª—É—á–∞–π –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ –æ—Å–Ω–æ–≤–Ω—ã—Ö
BACKUP_SOURCES = [
    "https://www.kommersant.ru/RSS/bank.xml",
    "https://www.rbc.ru/rss/economics.xml",
    "https://www.interfax.ru/rss.asp",
    "https://www.vestifinance.ru/rss/news"
]

# –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –≤ URL
RSS_SOURCES = [url.strip() for url in RSS_SOURCES]
BACKUP_SOURCES = [url.strip() for url in BACKUP_SOURCES]

BOT_TOKEN = os.getenv("BOT_TOKEN")
CHANNEL_ID = os.getenv("CHANNEL_ID")
MAX_POSTS_PER_DAY = 4
MAX_CONTENT_LENGTH = 800
MIN_CONTENT_LENGTH = 50

# User-Agent –¥–ª—è –æ–±—Ö–æ–¥–∞ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

# –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ ‚Üí —Ö–µ—à—Ç–µ–≥–∏ –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π —Ç–µ–º–∞—Ç–∏–∫–∏
KEYWORDS_TO_HASHTAGS = {
    "–±–∞–Ω–∫": ["#–±–∞–Ω–∫–∏", "#—Ñ–∏–Ω–∞–Ω—Å—ã", "#–±–∞–Ω–∫–æ–≤—Å–∫–∏–π–°–µ–∫—Ç–æ—Ä"],
    "–∫—Ä–µ–¥–∏—Ç": ["#–∫—Ä–µ–¥–∏—Ç", "#–∫—Ä–µ–¥–∏—Ç–æ–≤–∞–Ω–∏–µ", "#–∑–∞–π–º—ã"],
    "–∏–ø–æ—Ç–µ–∫–∞": ["#–∏–ø–æ—Ç–µ–∫–∞", "#–Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å", "#–∂–∏–ª—å–µ"],
    "–≤–∫–ª–∞–¥": ["#–≤–∫–ª–∞–¥—ã", "#–¥–µ–ø–æ–∑–∏—Ç—ã", "#—Å–±–µ—Ä–µ–∂–µ–Ω–∏—è"],
    "–∞–∫—Ü–∏—è": ["#–∞–∫—Ü–∏–∏", "#–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏", "#—Ñ–æ–Ω–¥–æ–≤—ã–π–†—ã–Ω–æ–∫"],
    "–æ–±–ª–∏–≥–∞—Ü–∏—è": ["#–æ–±–ª–∏–≥–∞—Ü–∏–∏", "#–±–æ–Ω–¥—Å", "#–¥–æ–ª–≥–æ–≤—ã–µ–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã"],
    "—Ä—É–±–ª—å": ["#—Ä—É–±–ª—å", "#–≤–∞–ª—é—Ç–∞", "#–∫—É—Ä—Å–†—É–±–ª—è"],
    "–¥–æ–ª–ª–∞—Ä": ["#–¥–æ–ª–ª–∞—Ä", "#USD", "#–≤–∞–ª—é—Ç–∞"],
    "–µ–≤—Ä–æ": ["#–µ–≤—Ä–æ", "#EUR", "#–≤–∞–ª—é—Ç–∞"],
    "–∏–Ω—Ñ–ª—è—Ü–∏—è": ["#–∏–Ω—Ñ–ª—è—Ü–∏—è", "#—Ü–µ–Ω—ã", "#—ç–∫–æ–Ω–æ–º–∏–∫–∞"],
    "–∫–ª—é—á–µ–≤–∞—è —Å—Ç–∞–≤–∫–∞": ["#–∫–ª—é—á–µ–≤–∞—è–°—Ç–∞–≤–∫–∞", "#–¶–ë", "#–ø—Ä–æ—Ü–µ–Ω—Ç–Ω–∞—è–°—Ç–∞–≤–∫–∞"],
    "–¶–ë": ["#–¶–µ–Ω—Ç—Ä–æ–±–∞–Ω–∫", "#—Ä–µ–≥—É–ª—è—Ç–æ—Ä", "#–±–∞–Ω–∫–†–æ—Å—Å–∏–∏"],
    "–§–†–°": ["#–§–†–°", "#–§–µ–¥–µ—Ä–∞–ª—å–Ω–∞—è–†–µ–∑–µ—Ä–≤–Ω–∞—è–°–∏—Å—Ç–µ–º–∞", "#–°–®–ê"],
    "–±–∏—Ä–∂–∞": ["#–±–∏—Ä–∂–∞", "#—Ç—Ä–µ–π–¥–∏–Ω–≥", "#—Ñ–æ–Ω–¥–æ–≤—ã–π–†—ã–Ω–æ–∫"],
    "–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞": ["#–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞", "#–±–∏—Ç–∫–æ–∏–Ω", "#–±–ª–æ–∫—á–µ–π–Ω"],
    "–Ω–µ—Ñ—Ç—å": ["#–Ω–µ—Ñ—Ç—å", "#–Ω–µ—Ñ—Ç—è–Ω—ã–µ–ö–æ—Ç–∏—Ä–æ–≤–∫–∏", "#—ç–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞"],
    "–≥–∞–∑": ["#–≥–∞–∑", "#—ç–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞", "#–ì–∞–∑–ø—Ä–æ–º"],
    "—ç–∫–æ–Ω–æ–º–∏–∫–∞": ["#—ç–∫–æ–Ω–æ–º–∏–∫–∞", "#–º–∞–∫—Ä–æ—ç–∫–æ–Ω–æ–º–∏–∫–∞", "#–í–í–ü"],
    "—Ä—ã–Ω–æ–∫": ["#—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π–†—ã–Ω–æ–∫", "#—Ä—ã–Ω–æ–∫", "#—Ç—Ä–µ–π–¥–µ—Ä—ã"],
    "–∏–Ω–≤–µ—Å—Ç": ["#–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏", "#–∏–Ω–≤–µ—Å—Ç–æ—Ä—ã", "#–∫–∞–ø–∏—Ç–∞–ª–æ–≤–ª–æ–∂–µ–Ω–∏—è"],
    "–ª–∏–∫–≤–∏–¥–Ω–æ—Å—Ç—å": ["#–ª–∏–∫–≤–∏–¥–Ω–æ—Å—Ç—å", "#–¥–µ–Ω–µ–∂–Ω—ã–π–†—ã–Ω–æ–∫", "#—Ñ–∏–Ω–∞–Ω—Å—ã"],
    "–¥–∏–≤–∏–¥–µ–Ω–¥": ["#–¥–∏–≤–∏–¥–µ–Ω–¥—ã", "#–¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å", "#–∞–∫—Ü–∏–æ–Ω–µ—Ä—ã"],
    "–∫—Ä–∏–∑–∏—Å": ["#–∫—Ä–∏–∑–∏—Å", "#—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–π–ö—Ä–∏–∑–∏—Å", "#—Ä–µ—Ü–µ—Å—Å–∏—è"],
    "—Å–∞–Ω–∫—Ü–∏–∏": ["#—Å–∞–Ω–∫—Ü–∏–∏", "#—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–µ–°–∞–Ω–∫—Ü–∏–∏", "#–º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ–û—Ç–Ω–æ—à–µ–Ω–∏—è"],
    "—Ä–µ–≥—É–ª—è—Ç–æ—Ä": ["#—Ä–µ–≥—É–ª—è—Ç–æ—Ä", "#–Ω–∞–¥–∑–æ—Ä", "#—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π–ù–∞–¥–∑–æ—Ä"],
}

# –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ —ç–º–æ–¥–∑–∏ –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π —Ç–µ–º–∞—Ç–∏–∫–∏
TOPIC_TO_EMOJI = {
    "–±–∞–Ω–∫": "üè¶",
    "–∫—Ä–µ–¥–∏—Ç": "üí≥",
    "–∏–ø–æ—Ç–µ–∫–∞": "üè†",
    "–≤–∫–ª–∞–¥": "üí∞",
    "–∞–∫—Ü–∏—è": "üìà",
    "–æ–±–ª–∏–≥–∞—Ü–∏—è": "üìä",
    "—Ä—É–±–ª—å": "‚ÇΩ",
    "–¥–æ–ª–ª–∞—Ä": "üíµ",
    "–µ–≤—Ä–æ": "üí∂",
    "–∏–Ω—Ñ–ª—è—Ü–∏—è": "üìâ",
    "–∫–ª—é—á–µ–≤–∞—è —Å—Ç–∞–≤–∫–∞": "üìå",
    "–¶–ë": "üá∑üá∫",
    "–§–†–°": "üá∫üá∏",
    "–±–∏—Ä–∂–∞": "üìä",
    "–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞": "‚Çø",
    "–Ω–µ—Ñ—Ç—å": "üõ¢Ô∏è",
    "–≥–∞–∑": "üî•",
    "—ç–∫–æ–Ω–æ–º–∏–∫–∞": "üåê",
    "—Ä—ã–Ω–æ–∫": "ü§ù",
    "–∏–Ω–≤–µ—Å—Ç": "üíº",
    "–ª–∏–∫–≤–∏–¥–Ω–æ—Å—Ç—å": "üíß",
    "–¥–∏–≤–∏–¥–µ–Ω–¥": "üéÅ",
    "–∫—Ä–∏–∑–∏—Å": "‚ö†Ô∏è",
    "—Å–∞–Ω–∫—Ü–∏–∏": "üö´",
    "—Ä–µ–≥—É–ª—è—Ç–æ—Ä": "üëÆ",
}


class NewsBot:
    def __init__(self, bot_token: str, channel_id: str):
        self.bot = Bot(token=bot_token)
        self.channel_id = channel_id
        self.session = None
        self.posted_hashes: Set[str] = set()
        self.failed_sources: Set[str] = set()
        self.load_hashes()

    def load_hashes(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ö–µ—à–∏ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π"""
        try:
            if os.path.exists('posted_hashes.txt'):
                with open('posted_hashes.txt', 'r', encoding='utf-8') as f:
                    self.posted_hashes = set(line.strip() for line in f)
                logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.posted_hashes)} —Ö–µ—à–µ–π –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏.")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ö–µ—à–µ–π: {e}")

    def save_hash(self, url: str, title: str):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ö–µ—à –Ω–æ–≤–æ—Å—Ç–∏"""
        url_hash = hashlib.md5(url.encode()).hexdigest()
        title_hash = hashlib.md5(title.encode()).hexdigest()
        combined = f"{url_hash}_{title_hash}"
        self.posted_hashes.add(combined)
        try:
            with open('posted_hashes.txt', 'a', encoding='utf-8') as f:
                f.write(combined + '\n')
        except Exception as e:
            logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ö–µ—à: {e}")

    def cleanup_old_hashes(self, days_to_keep: int = 30):
        """–û—á–∏—â–∞–µ—Ç —Å—Ç–∞—Ä—ã–µ —Ö–µ—à–∏ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è —Ä–∞–∑—Ä–∞—Å—Ç–∞–Ω–∏—è —Ñ–∞–π–ª–∞"""
        try:
            if os.path.exists('posted_hashes.txt'):
                if len(self.posted_hashes) > 1000:
                    self.posted_hashes = set(list(self.posted_hashes)[-500:])
                    with open('posted_hashes.txt', 'w', encoding='utf-8') as f:
                        for h in self.posted_hashes:
                            f.write(h + '\n')
                    logger.info("–û—á–∏—â–µ–Ω—ã —Å—Ç–∞—Ä—ã–µ —Ö–µ—à–∏")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ —Ö–µ—à–µ–π: {e}")

    def is_duplicate(self, url: str, title: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ —Ö–µ—à—É URL –∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞"""
        url_hash = hashlib.md5(url.encode()).hexdigest()
        title_hash = hashlib.md5(title.encode()).hexdigest()
        combined = f"{url_hash}_{title_hash}"
        return combined in self.posted_hashes

    @staticmethod
    def clean_text(text: str) -> str:
        """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç HTML, –∞–≤—Ç–æ—Ä–æ–≤, —Ä–µ–∫–ª–∞–º—ã –∏ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤"""
        if not text:
            return ""

        text = BeautifulSoup(text, "html.parser").get_text()
        text = html.unescape(text)

        # –£–¥–∞–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∞–≤—Ç–æ—Ä–µ, –∏—Å—Ç–æ—á–Ω–∏–∫–µ –∏ –¥–∞—Ç–µ
        patterns = [
            r'^\s*[–ê-–Ø][–∞-—è]+\s+[–ê-–Ø]\.[–ê-–Ø]\.?',
            r'^\s*[–ê-–Ø][–∞-—è]+(?:\s+[–ê-–Ø][–∞-—è]+)?\s*/\s*[–ê-–Ø][–∞-—è]+',
            r'^\s*‚Äî\s*[^\n]+',
            r'(–§–æ—Ç–æ|–ò–ª–ª—é—Å—Ç—Ä–∞—Ü–∏—è|–ò—Å—Ç–æ—á–Ω–∏–∫|–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π|–ê–≤—Ç–æ—Ä)[:\s].*?(?=\s*[–ê-–Ø])',
            r'^\s*[–ê-–Ø]\.\s*',
            r'^\s*–¶–µ–Ω—ã –Ω–∞.*?(?=\s*[–ê-–Ø])',
            r'^\s*–ü–æ.*?–Ω–∞ –ø—Ä–æ—à–ª–æ–π –Ω–µ–¥–µ–ª–µ.*?(?=\s*[–ê-–Ø])',
            r'^\s*\d{1,2}\s+[–ê-–Ø–∞-—è]+\s+\d{4}[^–ê-–Ø–∞-—è]*',
            r'–ï—â—ë –≤–∏–¥–µ–æ.*?(?=\s*[–ê-–Ø])',
            r'^\s*[–ê-–Ø–∞-—è]+\s+(?:–Ω–æ–≤–æ—Å—Ç–∏|–ø—Ä–µ—Å—Å-—Ä–µ–ª–∏–∑|—Å—Ç—Ä–∞—Ç–µ–≥–∏–∏)\b'
        ]

        for pattern in patterns:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)

        text = re.sub(r'\s+', ' ', text).strip()
        return text

    def extract_hashtags(self, title: str, content: str, creator: str = "") -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ö–µ—à—Ç–µ–≥–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ (–º–∞–∫—Å. 5)"""
        text = f"{title} {content} {creator}".lower()

        hashtags = set()
        for keyword, tags in KEYWORDS_TO_HASHTAGS.items():
            if keyword.lower() in text:
                hashtags.update(tags)

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π —Ç–µ–º–∞—Ç–∏–∫–∏
        extra = []
        if any(w in text for w in ["–±–∏—Ä–∂–∞", "—Ç—Ä–µ–π–¥–∏–Ω–≥", "–∏–Ω–≤–µ—Å—Ç"]):
            extra.append("#–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏")
        if any(w in text for w in ["–∫—Ä–∏–ø—Ç–æ", "–±–∏—Ç–∫–æ–∏–Ω", "–±–ª–æ–∫—á–µ–π–Ω"]):
            extra.append("#–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç—ã")
        if any(w in text for w in ["–Ω–µ—Ñ—Ç—å", "–≥–∞–∑", "—ç–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞"]):
            extra.append("#—ç–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞")
        if any(w in text for w in ["—Å–∞–Ω–∫—Ü–∏–∏", "—ç–º–±–∞—Ä–≥–æ", "–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è"]):
            extra.append("#–º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ–û—Ç–Ω–æ—à–µ–Ω–∏—è")

        hashtags.update(extra)
        return sorted(set(hashtags))[:5]

    def get_relevant_emoji(self, title: str, content: str, creator: str = "") -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π —ç–º–æ–¥–∑–∏"""
        text = f"{title} {content} {creator}".lower()
        for keyword, emoji in sorted(TOPIC_TO_EMOJI.items(), key=lambda x: len(x[0]), reverse=True):
            if keyword.lower() in text:
                return emoji
        return "üì∞"

    async def fetch_full_article_text(self, url: str) -> str:
        """–ü–∞—Ä—Å–∏—Ç –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ —Å –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            async with self.session.get(url, headers=HEADERS, timeout=15) as response:
                if response.status != 200:
                    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç–∞—Ç—å—é {url}: —Å—Ç–∞—Ç—É—Å {response.status}")
                    return ""
                html_text = await response.text()
                soup = BeautifulSoup(html_text, "html.parser")

                for elem in soup.find_all(['script', 'style', 'nav', 'header', 'footer', 'aside', 'advertisement', 'iframe']):
                    elem.decompose()

                domain = urlparse(url).netloc
                selectors = []

                if "finam.ru" in domain:
                    selectors = ['.article__body', '.content', 'article']
                elif "cbr.ru" in domain:
                    selectors = ['.content', '.text', 'article']
                elif "vedomosti.ru" in domain:
                    selectors = ['.article__body', '.article-body', 'article']
                elif "arb.ru" in domain:
                    selectors = ['.news-detail', '.content', 'article']
                elif "kommersant.ru" in domain:
                    selectors = ['.article__text', '.article-text', 'article']
                elif "rbc.ru" in domain:
                    selectors = ['.article__text', '.article__content', 'article']
                elif "banki.ru" in domain:
                    selectors = ['.news-text', '.article-content', 'article']
                else:
                    selectors = ['.article-content', '.post-content', '.entry-content', 
                                '.article__body', '.article-body', 'article', '.content']

                content = None
                for selector in selectors:
                    try:
                        if selector.startswith('.'):
                            content = soup.find(class_=selector[1:])
                        elif selector.startswith('#'):
                            content = soup.find(id=selector[1:])
                        else:
                            content = soup.find(selector)
                        if content:
                            break
                    except:
                        continue

                if not content:
                    all_texts = soup.find_all(text=True)
                    text_blocks = [t.parent for t in all_texts if len(t.strip()) > 50]
                    text_blocks.sort(key=lambda x: len(x.get_text()), reverse=True)
                    content = text_blocks[0] if text_blocks else soup.find('body')

                if content:
                    return self.clean_text(content.get_text())
                return ""
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —Å—Ç–∞—Ç—å–∏ {url}: {e}")
            return ""

    def smart_truncate(self, text: str, max_length: int = MAX_CONTENT_LENGTH) -> str:
        """–û–±—Ä–µ–∑–∞–µ—Ç —Ç–µ–∫—Å—Ç –ø–æ –≥—Ä–∞–Ω–∏—Ü–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π"""
        if len(text) <= max_length:
            return text
        truncated = text[:max_length + 1]
        last_end = max(truncated.rfind('.'), truncated.rfind('!'), truncated.rfind('?'))
        if last_end > max_length - 100:
            return truncated[:last_end + 1]
        else:
            return truncated[:max_length] + "‚Ä¶"

    def format_message(self, title: str, content: str, url: str, creator: str = "") -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ —Å HTML-—Ä–∞–∑–º–µ—Ç–∫–æ–π"""
        title = re.sub(r'\s*\d{1,2}\s+[–ê-–Ø–∞-—è]+\s+\d{4}\s*–≥–æ–¥–∞?\b', '', title, flags=re.IGNORECASE)
        
        full_text = self.clean_text(content)
        if full_text.startswith(title):
            full_text = full_text[len(title):].lstrip(":.- ")
        
        truncated_content = self.smart_truncate(full_text, MAX_CONTENT_LENGTH)

        sentences = re.split(r'(?<=[.!?])\s+', truncated_content)
        paragraphs = []
        current_paragraph = ""
        
        for sentence in sentences:
            if len(current_paragraph) + len(sentence) <= 120:
                current_paragraph += " " + sentence if current_paragraph else sentence
            else:
                if current_paragraph and len(current_paragraph.strip()) > 20:
                    paragraphs.append(current_paragraph.strip())
                current_paragraph = sentence
        
        if current_paragraph and len(current_paragraph.strip()) > 20:
            paragraphs.append(current_paragraph.strip())

        formatted_content = "\n\n".join(f"  {p}" for p in paragraphs)
        
        hashtags = self.extract_hashtags(title, content, creator)
        hashtag_line = " ".join(hashtags) if hashtags else ""
        emoji = self.get_relevant_emoji(title, content, creator)

        message = (
            f"<b>{emoji} {title}</b>\n\n"
            f"{formatted_content}\n\n"
            f"üëâ <a href='{url}'>–ß–∏—Ç–∞—Ç—å –¥–∞–ª–µ–µ</a>\n\n"
            f"{hashtag_line}"
        )
        return message

    async def fetch_feed(self, url: str) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ RSS-–ª–µ–Ω—Ç—ã —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ—á–∏—Å—Ç–∫–æ–π"""
        try:
            async with self.session.get(url, headers=HEADERS, timeout=15) as response:
                if response.status != 200:
                    logger.warning(f"–û—à–∏–±–∫–∞ HTTP {response.status} –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {url}")
                    self.failed_sources.add(url)
                    return []
                
                content = await response.text()
                feed = feedparser.parse(content)
                entries = []
                
                for entry in feed.entries:
                    title = entry.get("title", "").strip()
                    if "–≤–∏–¥–µ–æ" in title.lower() or "video" in title.lower():
                        continue

                    title = re.sub(r'^(.*?)(?:\s*-\s*[–ê-–Ø–∞-—è]+)*$', r'\1', title)
                    link = entry.get("link", "").strip()
                    description = entry.get("description", "") or entry.get("summary", "")

                    if "<![CDATA[" in description:
                        description = re.sub(r'<!\[CDATA\[(.*?)\]\]>', r'\1', description, flags=re.DOTALL)
                    
                    soup_desc = BeautifulSoup(description, "html.parser")
                    description = soup_desc.get_text()

                    description = re.sub(r'^\s*[–ê-–Ø]\.\s*[–ê-–Ø][–∞-—è]+\s*/\s*[–ê-–Ø][–∞-—è]+', '', description)
                    description = re.sub(r'^\s*[–ê-–Ø][–∞-—è]+\s+[–ê-–Ø]\.\s*', '', description)
                    description = re.sub(r'^\s*(?:–ê–≤—Ç–æ—Ä|–ò—Å—Ç–æ—á–Ω–∏–∫):?\s*[^\s]+\s*', '', description, flags=re.IGNORECASE)

                    creator = ""
                    if hasattr(entry, "tags") and entry.tags:
                        creator = " ".join([tag.term for tag in entry.tags[:2]])
                    elif hasattr(entry, "author"):
                        creator = entry.author
                    elif hasattr(entry, "dc") and hasattr(entry.dc, "creator"):
                        creator = entry.dc.creator
                    elif hasattr(entry, "creator"):
                        creator = entry.creator

                    if title and link:
                        entries.append({
                            "title": title,
                            "url": link,
                            "content": description,
                            "creator": creator or "",
                            "source": url
                        })
                
                logger.info(f"–ü–æ–ª—É—á–µ–Ω–æ {len(entries)} –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ {urlparse(url).netloc}")
                return entries
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {url}: {e}")
            self.failed_sources.add(url)
            return []

    async def publish_post(self, title: str, content: str, url: str, creator: str = "") -> bool:
        """–ü—É–±–ª–∏–∫—É–µ—Ç –ø–æ—Å—Ç —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        if self.is_duplicate(url, title):
            logger.info(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ (–¥—É–±–ª–∏–∫–∞—Ç): {title[:50]}...")
            return False

        full_text = await self.fetch_full_article_text(url)
        use_text = full_text if full_text.strip() else content

        if len(self.clean_text(use_text)) < MIN_CONTENT_LENGTH:
            logger.info(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ (–º–∞–ª–æ —Ç–µ–∫—Å—Ç–∞): {title[:50]}...")
            return False

        max_retries = 3
        for attempt in range(max_retries):
            try:
                message = self.format_message(title, use_text, url, creator)
                await self.bot.send_message(
                    chat_id=self.channel_id,
                    text=message,
                    parse_mode='HTML',
                    disable_web_page_preview=False
                )
                logger.info(f"‚úÖ –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ: {title[:50]}...")
                self.save_hash(url, title)
                return True
            except error.RetryAfter as e:
                logger.warning(f"‚ö†Ô∏è –ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç, –æ–∂–∏–¥–∞–Ω–∏–µ {e.retry_after} —Å–µ–∫...")
                await asyncio.sleep(e.retry_after)
            except error.TimedOut:
                logger.warning(f"‚ö†Ô∏è –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏, –ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries}")
                await asyncio.sleep(2 ** attempt)
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ '{title}' (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}): {e}")
                if attempt == max_retries - 1:
                    return False
                await asyncio.sleep(2 ** attempt)
        
        return False

    def generate_post_schedule(self) -> List[datetime]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ: –ø–µ—Ä–≤–∞—è –ø—É–±–ª–∏–∫–∞—Ü–∏—è —Å—Ä–∞–∑—É, –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Å–ª—É—á–∞–π–Ω–æ"""
        now = datetime.now()
        times = [now]  # –ü–µ—Ä–≤–∞—è –ø—É–±–ª–∏–∫–∞—Ü–∏—è —Å—Ä–∞–∑—É
        
        for _ in range(MAX_POSTS_PER_DAY - 1):
            random_hours = random.randint(1, 6)
            random_minutes = random.randint(0, 59)
            next_time = now + timedelta(hours=random_hours, minutes=random_minutes)
            
            if next_time.hour >= 22:
                next_time = next_time.replace(hour=21, minute=0)
            
            times.append(next_time)
            now = next_time
        
        return sorted(times)

    def avoid_consecutive_sources(self, posts: List[Dict]) -> List[Dict]:
        """–ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ—Ç –ø–æ—Å—Ç—ã, —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ >2 –ø–æ–¥—Ä—è–¥ –∏–∑ –æ–¥–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        if len(posts) < 3:
            return posts

        source_groups = {}
        for post in posts:
            source = post["source"]
            if source not in source_groups:
                source_groups[source] = []
            source_groups[source].append(post)

        sorted_groups = sorted(source_groups.items(), key=lambda x: len(x[1]), reverse=True)
        
        result = []
        
        while any(groups for _, groups in sorted_groups):
            for i, (source, group) in enumerate(sorted_groups):
                if group and (not result or result[-1]["source"] != source):
                    result.append(group.pop(0))
                    break
            else:
                for i, (source, group) in enumerate(sorted_groups):
                    if group:
                        result.append(group.pop(0))
                        break
        
        return result

    async def run(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –±–æ—Ç–∞ —Å –æ–±—Ö–æ–¥–æ–º –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫"""
        connector = aiohttp.TCPConnector(limit=5, ttl_dns_cache=300)
        timeout = aiohttp.ClientTimeout(total=20)
        
        async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
            self.session = session
            self.cleanup_old_hashes()

            all_news = []
            seen_urls = set()

            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
            for source in RSS_SOURCES:
                try:
                    if source in self.failed_sources:
                        continue
                        
                    news = await self.fetch_feed(source)
                    for item in news:
                        if item["url"] not in seen_urls:
                            all_news.append(item)
                            seen_urls.add(item["url"])
                    await asyncio.sleep(2)  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ {source}: {e}")
                    self.failed_sources.add(source)

            # –ï—Å–ª–∏ –Ω–æ–≤–æ—Å—Ç–µ–π –º–∞–ª–æ, –ø—Ä–æ–±—É–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
            if len(all_news) < MAX_POSTS_PER_DAY:
                logger.info("–ü—Ä–æ–±—É–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏...")
                for backup_source in BACKUP_SOURCES:
                    try:
                        if backup_source in self.failed_sources:
                            continue
                            
                        news = await self.fetch_feed(backup_source)
                        for item in news:
                            if item["url"] not in seen_urls:
                                all_news.append(item)
                                seen_urls.add(item["url"])
                        await asyncio.sleep(2)
                    except Exception as e:
                        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ä–µ–∑–µ—Ä–≤–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ {backup_source}: {e}")
                        self.failed_sources.add(backup_source)

            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
            all_news = [n for n in all_news if not self.is_duplicate(n["url"], n["title"])]
            
            if not all_news:
                logger.warning("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏")
                return
                
            if len(all_news) < MAX_POSTS_PER_DAY:
                logger.warning(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: {len(all_news)} –∏–∑ {MAX_POSTS_PER_DAY}")

            random.shuffle(all_news)
            selected_posts = all_news[:MAX_POSTS_PER_DAY]
            final_posts = self.avoid_consecutive_sources(selected_posts)

            schedule = self.generate_post_schedule()
            logger.info(f"–†–∞—Å–ø–∏—Å–∞–Ω–∏–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–π: {', '.join(t.strftime('%H:%M') for t in schedule)}")

            # –ü—É–±–ª–∏–∫–∞—Ü–∏—è –ø–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—é
            published_count = 0
            for i, post_time in enumerate(schedule):
                now = datetime.now()
                if i > 0:
                    if now < post_time:
                        wait_seconds = (post_time - now).total_seconds()
                        logger.info(f"‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ –¥–æ {post_time.strftime('%H:%M')} ‚Äî {int(wait_seconds)} —Å–µ–∫...")
                        await asyncio.sleep(wait_seconds)

                if i < len(final_posts):
                    post = final_posts[i]
                    success = await self.publish_post(
                        title=post["title"],
                        content=post["content"],
                        url=post["url"],
                        creator=post.get("creator", "")
                    )
                    if success:
                        published_count += 1
                else:
                    logger.warning(f"–ù–µ—Ç –ø–æ—Å—Ç–∞ –¥–ª—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –≤ —Å–ª–æ—Ç {i}")

            logger.info(f"üîö –¶–∏–∫–ª –∑–∞–≤–µ—Ä—à—ë–Ω. –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ {published_count} –∏–∑ {len(schedule)} –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—É–±–ª–∏–∫–∞—Ü–∏–π.")


# === –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞ ===
async def main():
    if not BOT_TOKEN:
        logger.critical("‚ùå BOT_TOKEN –Ω–µ –∑–∞–¥–∞–Ω –≤ .env —Ñ–∞–π–ª–µ!")
        return
    if not CHANNEL_ID:
        logger.critical("‚ùå CHANNEL_ID –Ω–µ –∑–∞–¥–∞–Ω –≤ .env —Ñ–∞–π–ª–µ!")
        return

    bot = NewsBot(BOT_TOKEN, CHANNEL_ID)
    try:
        await bot.run()
    except Exception as e:
        logger.critical(f"üí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")


if __name__ == "__main__":
    asyncio.run(main())